{"pages":[],"posts":[{"title":"Centos-NTP","text":"前言 NTP时钟同步方式说明 NTP在linux下有两种时钟同步方式，分别为直接同步和平滑同步： 直接同步 使用ntpdate命令进行同步，直接进行时间变更。如果服务器上存在一个12点运行的任务，当前服务器时间是13点，但标准时间时11点，使用此命令可能会造成任务重复执行。因此使用ntpdate同步可能会引发风险，因此该命令也多用于配置时钟同步服务时第一次同步时间时使用。 平滑同步 使用ntpd进行时钟同步，可以保证一个时间不经历两次，它每次同步时间的偏移量不会太陡，是慢慢来的，这正因为这样，ntpd平滑同步可能耗费的时间比较长。 标准时钟同步服务 网站包含全球的标准时间同步服务，也包括对中国时间的同步，对应的URL 为 cn.pool.ntp.org，在其中也描述了ntp配置文件中的建议写法： server 1.cn.pool.ntp.org server 3.asia.pool.ntp.org server 2.asia.pool.ntp.org 环境 节点 IP master 192.168.0.100 slave1 192.168.0.101 slave2 192.168.0.102 slave3 192.168.0.103 master 配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# 安装 ntpyum install ntp# 启动 ntp 服务# 设置开机自启systemctl start ntpdsystemctl enable ntpd# 配置 ntp.confvi /etc/ntp.conf# For more information about this file, see the man pages# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).driftfile /var/lib/ntp/drift# Permit time synchronization with our time source, but do not# permit the source to query or modify the service on this system.restrict default nomodify notrap nopeer noquery# Permit all access over the loopback interface. This could# be tightened as well, but to do so would effect some of# the administrative functions.restrict 127.0.0.1restrict ::1# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburstserver 2.cn.pool.ntp.orgserver 1.asia.pool.ntp.orgserver 2.asia.pool.ntp.org#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client# 允许上层时间服务器主动修改本机时间restrict 2.cn.pool.ntp.org nomodify notrap noqueryrestrict 1.asia.pool.ntp.org nomodify notrap noqueryrestrict 2.asia.pool.ntp.org nomodify notrap noquery# Enable public key cryptography.#cryptoincludefile /etc/ntp/crypto/pw# Key file containing the keys and key identifiers used when operating# with symmetric key cryptography. keys /etc/ntp/keys# Specify the key identifiers which are trusted.#trustedkey 4 8 42# Specify the key identifier to use with the ntpdc utility.#requestkey 8# Specify the key identifier to use with the ntpq utility.#controlkey 8# Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats# Disable the monitoring facility to prevent amplification attacks using ntpdc# monlist command when default restrict does not include the noquery flag. See# CVE-2013-5211 for more details.# Note: Monitoring will not be disabled with the limited restriction flag.disable monitor# 重启 ntpd systemctl restart ntpd slave 设置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# 安装 ntp 并启动服务和开机自启# 和 master 设置方式一样# 配置 ntp.confvi /etc/ntp.conf# For more information about this file, see the man pages# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).driftfile /var/lib/ntp/drift# Permit time synchronization with our time source, but do not# permit the source to query or modify the service on this system.restrict default nomodify notrap nopeer noquery# Permit all access over the loopback interface. This could# be tightened as well, but to do so would effect some of# the administrative functions.restrict 127.0.0.1restrict ::1# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).#server 0.centos.pool.ntp.org iburstserver 192.168.0.100restrict 192.168.0.100 nomodify notrap noquery#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client# Enable public key cryptography.#cryptoincludefile /etc/ntp/crypto/pw # Key file containing the keys and key identifiers used when operating# with symmetric key cryptography. keys /etc/ntp/keys # Specify the key identifiers which are trusted.#trustedkey 4 8 42# Specify the key identifier to use with the ntpdc utility.#requestkey 8# Specify the key identifier to use with the ntpq utility.#controlkey 8# Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats# Disable the monitoring facility to prevent amplification attacks using ntpdc# monlist command when default restrict does not include the noquery flag. See# CVE-2013-5211 for more details.# Note: Monitoring will not be disabled with the limited restriction flag.disable monitor# 重启 ntpd 使用方式ntpq -p 查看网络中的NTP服务器，同时显示客户端和每个服务器的关系 1234567891011121314# ntpq -p 查看网络中的NTP服务器，同时显示客户端和每个服务器的关系[root@master ~]# ntpq -p remote refid st t when poll reach delay offset jitter==============================================================================+ntp7.flashdance 194.58.202.20 2 u 40 64 377 297.202 2.506 6.218*ntp.nic.kz .SHM. 1 u 47 64 377 433.554 -74.865 7.147+send.mx.cdnetwo 216.239.35.8 2 u 33 64 377 262.368 -122.93 25.064 localhost .INIT. 16 l - 512 0 0.000 0.000 0.000 [root@slave1 ~]# ntpq -p remote refid st t when poll reach delay offset jitter==============================================================================*master 80.241.0.72 2 u 58 64 377 0.306 14.913 1.479 localhost .INIT. 16 l - 512 0 0.000 0.000 0.000 12345678910# ntpstat 查看时间同步状态[root@master ~]# ntpstatsynchronised to NTP server (80.241.0.72) at stratum 2 time correct to within 343 ms polling server every 64 s[root@slave1 ~]# ntpstatsynchronised to NTP server (192.168.0.100) at stratum 3 time correct to within 347 ms polling server every 64 s unsplash-logotommy boudreau","link":"/posts/87856f92/"},{"title":"RabbitMQ基础概念","text":"消息队列相关消息队列是什么wiki上给出的定义为： 在计算机科学中，消息队列（英语：Message queue）是一种进程间通信或同一进程的不同线程间的通信方式，软体的贮列用来处理一系列的输入，通常是来自使用者。消息队列提供了异步的通信协议，每一个贮列中的纪录包含详细说明的资料，包含发生的时间，输入装置的种类，以及特定的输入参数，也就是说：消息的发送者和接收者不需要同时与消息队列交互。消息会保存在队列中，直到接收者取回它。[1] 一个 WIMP 环境像是 Microsoft Windows，借由优先的某些形式（通常是事件的时间或是重要性的顺序）来储存使用者产生的事件到一个 事件贮列 中。系统把每个事件从事件贮列中传递给目标的应用程式。 实现方式实际上，消息队列常常保存在链表结构中。[2]拥有权限的进程可以向消息队列中写入或读取消息。 RabbitMQ的特点官网上描述的特点是: RabbitMQ是部署最广泛的开源消息代理。 RabbitMQ轻量级，易于在内部和云中部署。它支持多种消息传递协议。RabbitMQ部署在分布式和集群中，以满足高规模，高可用性要求。 RabbitMQ可运行在多数操作系统和云平台中，并为大多数流行的语言提供各种开发工具。 RabbitMQ中的概念模型消息模型所有 MQ 产品从模型抽象上来说都是一样的过程：消费者（consumer）订阅某个队列。生产者（producer）创建消息，然后发布到队列（queue）中，最后将消息发送到监听的消费者。 RabbitMQ 基本概念上面只是最简单抽象的描述，具体到 RabbitMQ 则有更详细的概念需要解释。上面介绍过 RabbitMQ 是 AMQP 协议的一个开源实现，所以其内部实际上也是 AMQP 中的基本概念： Message 消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 Publisher 消息的生产者，也是一个向交换器发布消息的客户端应用程序。 Exchange 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。 Binding 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。 Queue 消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Connection 网络连接，比如一个TCP连接。 Channel 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。 Consumer 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。 Virtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。 Broker表示消息队列服务器实体。 AMQP 中的消息路由AMQP 中消息的路由过程和 Java 开发者熟悉的 JMS 存在一些差别，AMQP 中增加了 Exchange 和 Binding 的角色。生产者把消息发布到 Exchange 上，消息最终到达队列并被消费者接收，而 Binding 决定交换器的消息应该发送到那个队列。 Exchange的类型Exchange分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。headers 匹配 AMQP 消息的 header 而不是路由键，此外 headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型： direct 消息中的路由键（routing key）如果和 Binding 中的 binding key 一致， 交换器就将消息发到对应的队列中。路由键与队列名完全匹配，如果一个队列绑定到交换机要求路由键为“dog”，则只转发 routing key 标记为“dog”的消息，不会转发“dog.puppy”，也不会转发“dog.guard”等等。它是完全匹配、单播的模式。 fanout 每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout 交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息都会被转发到与该交换器绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。 topic topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符：符号“#”和符号“*”。#匹配0个或多个单词，*匹配不多不少一个单词。 unsplash-logoAndrew Ly","link":"/posts/1f364dfa/"},{"title":"hexo&icuras配置指南","text":"使用 hexo 写了大概一年左右的博客，期间参考网上各种教程给博客来了个大装修。但就像是做衣服，使用不同的布料，缝缝补补给生硬凑出来的一样，最终成了四不像。非但如此，因为各种插件和没用的渲染，博客的打开速度简直惨不忍睹。 所以，打算翻新一下，从头开始再搭一个。这次的原则就是简洁，不再搞那些没用的花里胡哨的东西。配置过程记录如下。所有的配置文件在 github仓库。 基础篇hexo 是一个开源的静态博客框架，基本开箱即用。安装见官网。 站点的配置文件在博客的根目录下的 _config.yml 文件中，具体各个字段的意义以及配置方法参见 hexo官网 主题配置体验了蛮多的 hexo 主题，最终挑选了 icarus。 主要的原因是： 界面简洁，适配各个终端 集成度高，内置的插件基本满足日常使用 开发团队较为活跃，目前还在持续更新 主题的配置基本都在 themes/icarus/_config.yml 文件中。 默认的主题配置包含以下内容： 站点首选项和页面元数据 顶部导航栏链接 底部页脚链接 文章显示设置 评论、分享和搜索插件设置 侧边小部件设置 其他显示和分析插件 CDN 设置 参见各字段的含义进行配置即可。 使用技巧阅读更多在你的文章中添加 &lt;!-- more --&gt; 标签。该标签之前的帖子内容将被标记为摘录，而该标签之后的内容将不会显示在索引页面上。 效果 #### 添加缩略图 在配置文件中启用缩略图（默认启用） _config.yml12article: thumbnail: true 然后，在文章的开头提供URL或图像文件的路径 post.md1234title: your titlethumbnail: /gallery/thumbnails/desert.jpg---Post content... 图片路径 图片的路径应该是相对于站点目录的相对路径。举例来说，如果你想用下面的图片来当作缩略图 1&lt;your blog&gt;/source/gallery/image.jpg 你需要使用如下的图片路径 1/gallery/image.jpg 另外，建议你将所有图片放在 _posts 文件夹中的 asset 文件夹下。 效果图 #### 添加目录 在配置文件中启用目录（默认启用） _config.yml1234widgets: - type: toc position: left 然后，在文章的开头添加toc: true 1234title: Table of Contents Exampletoc: true---Post content... 效果 ## 插件篇 主题内置插件评论内置评论插件 官方配置文档，我使用的是 valine 效果 #### 打赏 官方配置文档 效果 ### 其他插件 在博客的站点目录下安装插件。 hexo-abbrlinkhexo 默认的永久链接生成方案是 年/月/日/标题。这样一来，链接不但很长，而且，如果你是中文用户，生成的链接中还将包含中文，这无疑会留下很多麻烦。 安装 1npm install hexo-abbrlink --save 配置 修改站点 _config.yml _config.yml1permalink: posts/:abbrlink/ 还需要设置两个参数 12alg -- Algorithm (currently support crc16 and crc32, which crc16 is default)rep -- Represent (the generated link could be presented in hex or dec value) 如果你不想用默认的参数，需要在站点 _config.yml 中添加 _config.yml12345# abbrlink configabbrlink: alg: crc32 #support crc16(default) and crc32 rep: hex #support dec(default) and hex hexo-generator-feed生成 Atom 1.0 或者 RSS 2.0 为自己的博客提供订阅功能。 安装 1npm install hexo-generator-feed --save 配置站点目录_config.yml _config.yml123456789101112feed: type: atom path: atom.xml limit: 20 hub: content: content_limit: 140 content_limit_delim: ' ' order_by: -date icon: icon.png autodiscovery: true template: 参数的具体含义见官方仓库。 验证配置是否成功 执行 hexo g，查看一下 public 目录，如果有 atom.xml 文件，则表明配置成功。 配置RSS 这里以 icarus 主题为例，给rss添加链接，修改 _config.yaml _config.yml 123RSS: icon: fas fa-rss url: /atom.xml 效果 #### [hexo-deployer-git](https://github.com/hexojs/hexo-deployer-git) 如果你使用 git 部署博客的话，需要安装此插件。 安装 1npm install hexo-deployer-git --save 配置 编辑站点配置文件 _config.yml1234567891011deploy: - type: git repo: git@github.com:&lt;username&gt;/&lt;username&gt;.github.io.git branch: master - type: git repo: git@github.com:&lt;username&gt;/&lt;username&gt;.github.io.git branch: src extend_dirs: / ignore_hidden: false ignore_pattern: public: . 具体的参数请参考官方说明。 sharejs一键分享到微博、QQ空间、QQ好友、微信、腾讯微博、豆瓣、Facebook、Twitter、Linkedin、Google+、点点等 安装 1npm install social-share.js 编辑主题配置文件 _config.yml12share: type: sharejs 效果 ## 脚本篇 在博客站点目录下新建文件夹 scripts，所有的脚本都放在此目录下。 新建博客自动打开当你使用 hexo new 新建一篇博客时将自动打开markdown编辑器 新建 trigger.js 脚本，内容如下 1234var exec = require('child_process').exec;hexo.on('new', function(data){ exec('open -a \"your/app/path\" ' + data.path);}); 把脚本中 your/app/path 替换成你的 markdown 编辑器。 unsplash-logoTomáš Malík","link":"/posts/684e7cbb/"},{"title":"HA高可用集群搭建","text":"之前已经搭建好了一个集群，传送 这次在原来的基础上做一个双master的高可用集群。 Hadoop HA简介及工作原理Hadoop NameNode官方开始支持HA集群默认是从2.0开始，之前的版本均是不支持NameNode HA的高可用的。 Hadoop HA简介Hadoop-HA集群运作机制介绍 HA即高可用（7*24小时不中断服务） 实现高可用最关键的是消除单点故障 分成各个组件的HA机制——HDFS的HA、YARN的HA HDFS的HA机制详解通过双namenode消除单点故障，以下为双namenode协调工作的特点： A、元数据管理方式需要改变： 内存中各自保存一份元数据 Edits日志只能有一份，只有Active状态的namenode节点可以做写操作 两个namenode都可以读取edits 共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现） B、需要一个状态管理功能模块 实现了一个zkfailover，常驻在每一个namenode所在的节点 每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识 当需要进行状态切换时，由zkfailover来负责切换 切换时需要防止brain split现象的发生 Hadoop HA工作原理图例HDFS的HA架构 使用 Active NameNode，Standby NameNode 两个结点解决单点问题，两个结点通过JounalNode 共享状态，采用ZKFC选举Active实时监控集群状态，自动进行故障备援。 Active NameNode：接受 client 的 RPC 请求并处理，同时写自己的 Editlog 和共享存储上的 Editlog，接收 DataNode 的 Block report, block location updates 和 heartbeat； Standby NameNode：同样会接到来自 DataNode 的 Block report, block location updates 和heartbeat，同时会从共享存储的 Editlog 上读取并执行这些 log 操作，使得自己的 NameNode 中的元数据（Namespcae information + Block locations map）都是和 Active NameNode 中的元数据是同步的。所以说 Standby 模式的 NameNode 是一个热备（Hot Standby NameNode），一旦切换成 Active 模式，马上就可以提供 NameNode 服务 JounalNode：用于Active NameNode ， Standby NameNode 同步数据，本身由一组 JounnalNode 结点组成，该组结点基数个，支持 Paxos 协议，保证高可用，是 CDH5 唯一支持的共享方式（相对于 CDH4 促在NFS共享方式） ZKFC：监控NameNode进程，自动备援。 YARN的HA架构 ResourceManager HA由一对Active，Standby结点构成，通过RMStateStore 存储内部数据和主要应用的数据及标记。 支持可替代的RMStateStore实现方式如下： 基于内存的MemoryRMStateStore 基于文件系统的FileSystemRMStateStore 基于 zookeeper的ZKRMStateStoreResourceManager HA 的架构模式同NameNode HA的架构模式基本一致，数据共享由 RMStateStore，而ZKFC成为ResourceManager进程的一个服务，非独立存在。 Hadoop HA解决方案架构Hadoop中的HDFS、MapReduce和YARN的单点故障解决方案架构是完全一致的。 手动模式：指由管理员通过命令进行主备切换，这通常在服务升级时有用。 自动模式：自动模式可降低运维成本并自动切换，但存在潜在危险，如脑裂。 本文将重点介绍下自动模式切换的部署方式。 什么是脑裂：脑裂是Hadoop2.X版本后出现的全新问题，从字面意思我们可以理解为“大脑分裂”；我们想一下，当一个正常人，突然出现有了两个大脑，而且这两个大脑都有自己的意识，对于这个人来说肯定是灾难性问题。同理，在Hadoop中，为了防止单点失效问题而出现了两个namenode（HA机制），这两个namenode正常情况下是起到一个失效，另一个代替的作用，但在实际运行过程中很有可能出现两个namenode同时服务于整个集群的情况，这种情况称之为脑裂。 为什么会出现脑裂：脑裂通常发生在主从namenode切换时，由于ActiveNameNode的网络延迟、设备故障等问题，另一个NameNode会认为活跃的NameNode成为失效状态，此时StandbyNameNode会转换成活跃状态，此时集群中将会出现两个活跃的namenode。因此，可能出现的因素有网络延迟、心跳故障、设备故障等。 怎么解决脑裂问题：1.新增一条心跳线，防止namennode状态无法正常传达。2.使用隔离机制，通过调用活跃节点中的隔离方法，让其主动转换为standby状态，如果该方法失效则使用远程调用执行kill -9命令杀死相应进程，如果该方法仍然无法成功隔离，管理人员可以事先在每台namenode节点中编写一个shell脚本，当出现脑裂问题时，执行该脚本来切断电源，已达到隔离目的。 其他平台HA类似 HA环境准备各主机IP规划 主机名 IP地址 操作系统 安装软件 运行进程 sgmaster1 192.168.1.100 centos7.4 jdk、hadoop、zookeeper、hbase、spark NameNode、DFSZKFailoverController(zkfc)、ResourceManager、HMaster、HRegionServer、Master sgmaster2 192.168.1.101 centos7.4 jdk、hadoop、zookeeper、hbase、spark NameNode、DFSZKFailoverController(zkfc)、ResourceManager、HMaster、HRegionServer、Master sgslaver1 192.168.1.102 centos7.4 jdk、hadoop、zookeeper、hbase、spark DataNode、NodeManager、JournalNode、QuorumPeerMain、Worker、HRegionServer sgslaver2 192.168.1.103 centos7.4 jdk、hadoop、zookeeper、hbase、spark DataNode、NodeManager、JournalNode、QuorumPeerMain、Worker、HRegionServer sgslaver3 192.168.1.104 centos7.4 jdk、hadoop、zookeeper、hbase、spark DataNode、NodeManager、JournalNode、QuorumPeerMain、Worker、HRegionServer 准备 添加 hosts ssh 互信 软件包安装配置 以上内容可参考另外一篇博文 HA配置Zookeeper配置zoo.cfg 1234567891011121314151617181920212223242526272829303132vim $ZOOKEEPER_HOME/conf/zoo.cfg# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/data/zookeeperdataLogDir=/data/zookeeper/logs# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60#admin.serverPort=8090# Be sure to read the maintenance section of the# administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to \"0\" to disable auto purge feature#autopurge.purgeInterval=1server.1=sgmaster1:2888:3888server.2=sgmaster2:2888:3888server.3=sgslaver1:2888:3888server.4=sgslaver2:2888:3888server.5=sgslaver3:2888:3888 创建myid 12# 分别对应1-5，各自执行即可。echo \"1\" &gt; /data/zookeeper/myid 启动或关闭12zkServer.sh startzkServer.sh stop Hadoop配置core-site.xml 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;sgmaster1:2181,sgmaster2:2181,sgslaver1:2181,sgslaver2:2181,sgslaver3:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;15000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 123456789101112131415161718 &lt;value&gt;shell(/bin/true)&lt;/value&gt; &lt;/property&gt; &lt;!--SSH私钥 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!--SSH超时时间 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;!--Journal Node文件存储地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/data/hadoop/journal&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 1234567891011121314151617181920 &lt;/property&gt; &lt;!-- 分别指定RM的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;sgmaster1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;sgmaster2&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk集群地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;sgmaster1:2181,sgmaster2:2181,sgslaver1:2181,sgslaver2:2181,sgslaver3:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; workers 123sgslaver1sgslaver2sgslaver3 启动及维护初始化zookeeper并启动集群 启动zookeeper节点：从节点分别执行 1zkServer.sh start 格式化zookeeper节点：sgmaster1执行 1hdfs zkfc -formatZK 初始化hadoop并启动集群 启动journalnode节点：从节点分别执行 1hdfs --daemon start journalnode 格式化namenode：sgmaster1上执行 1hdfs namenode -format 节点sgmaster1 12345# 验证，显示NameNode和DataNodestart-dfs.sh# 验证，显示ResourceManager和NodeManagerstart-yarn.sh 节点sgmaster2 12hdfs namenode -bootstrapStandbyhadoop-daemon.sh start namenode 此时sre01和sre02均处于standby状态。 启动zkfc服务：master上分别执行 1hdfs --daemon start zkfc 健康状态检查：运行状态说明。 启动zkfc服务后，sgmaster1和sgmaster2会自动选举出active节点。 此时一个节点为active状态，另一个处于standby状态。 在浏览器中打开 NamaNode UI、Yarn UI，可以产看主节点和备用主节点的状态 HA故障自动切换测试主节点—&gt;备用主节点 kill掉主节点的namenode，查看备用主节点的namenode状态是否切换为active； kill掉主节点的ResourceManager，查看备用主节点的ResourceManager是否切换为active； ######备用主节点—&gt;主节点 若上述操作执行成功，那么再测试反向故障自动转移 先启动被杀死的原主节点的namenode和ResourceManager 123hadoop-daemon.sh start namenode yarn-daemon.sh start resourcemanager 再kill备用主节点的namenode和ResourceManager，查看主节点的状态，若能切换为active，那么Hadoop HA高可用集群搭建完成。 Hbase配置hbase-env.sh 12345678910111213141516171819202122232425262728293031# A string representing this instance of hbase. $USER by default.# export HBASE_IDENT_STRING=$USER# The scheduling priority for daemon processes. See 'man nice'.# export HBASE_NICENESS=10# The directory where pid files are stored. /tmp by default.# export HBASE_PID_DIR=/var/hadoop/pidsexport HBASE_PID_DIR=/data/hbase/pids# Seconds to sleep between slave commands. Unset by default. This# can be useful in large clusters, where, e.g., slave rsyncs can# otherwise arrive faster than the master can service them.# export HBASE_SLAVE_SLEEP=0.1# Tell HBase whether it should manage it's own instance of ZooKeeper or not.# export HBASE_MANAGES_ZK=trueexport HBASE_MANAGES_ZK=false# The default log rolling policy is RFA, where the log file is rolled as per the size defined for the# RFA appender. Please refer to the log4j.properties file to see more details on this appender.# In case one needs to do log rolling on a date change, one should set the environment property# HBASE_ROOT_LOGGER to \"&lt;DESIRED_LOG LEVEL&gt;,DRFA\".# For example:# HBASE_ROOT_LOGGER=INFO,DRFA# The reason for changing default to RFA is to avoid the boundary case of filling out disk space as# DRFA doesn't put any cap on the log size. Please refer to HBase-5655 for more context.# Tell HBase whether it should include Hadoop's lib when start up,# the default value is false,means that includes Hadoop's lib.# export HBASE_DISABLE_HADOOP_CLASSPATH_LOOKUP=\"true\" hbase-site.xml 1234567891011121314 &lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;16010&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.regionserver.info.port&lt;/name&gt; &lt;value&gt;16030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;完全分布式式必须为false&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; regionservers 12345sgmaster1sgmaster2sgslaver1sgslaver2sgslaver3 创建pid文件保存目录1mkdir -p /data/hbase/pids 启动 在主节点上启动HBase 这里的主节点是指NameNode状态为active的节点 12# 查看HMaster、Regionserver进程是否启动start-hbase.sh 在备用主节点启动HMaster进程 1hbase-daemon.sh start master HA高可用测试在浏览器中打开 Web UI，可以查看主节点和备用主节点的状态 主节点—&gt;备用主节点 这里的主节点指使用start-hbase.sh命令启动HBase集群的机器 kill掉主节点的HMaster进程，在浏览器中查看备用主节点的HBase是否切换为active； 若上述操作成功，则在主节点启动被杀死的HMaster进程： 1hbase-daemon.sh start master 然后，kill掉备用主节点的HMaster进程，在浏览器中查看主节点的HBase是否切换为active，若操作成功，则HBase高可用集群搭建完成； Spark配置spark-env.sh 12345678910111213141516171819202122232425# - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. \"-Dx=y\")# - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. \"-Dx=y\")export SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=sgmaster1:2181,sgmaster2:2181,sgslaver1:2181,sgslaver2:2181,sgslaver3:2181 -Dspark.deploy.zookeeper.dir=/spark\"# - SPARK_DAEMON_CLASSPATH, to set the classpath for all daemons# - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers# Generic options for the daemons used in the standalone deploy mode# - SPARK_CONF_DIR Alternate conf dir. (Default: ${SPARK_HOME}/conf)# - SPARK_LOG_DIR Where log files are stored. (Default: ${SPARK_HOME}/logs)SPARK_LOG_DIR=/data/spark/logs# - SPARK_PID_DIR Where the pid file is stored. (Default: /tmp)# - SPARK_IDENT_STRING A string representing this instance of spark. (Default: $USER)# - SPARK_NICENESS The scheduling priority for daemons. (Default: 0)# - SPARK_NO_DAEMONIZE Run the proposed command in the foreground. It will not output a PID file.# Options for native BLAS, like Intel MKL, OpenBLAS, and so on.# You might get better performance to enable these options if using native BLAS (see SPARK-21305).# - MKL_NUM_THREADS=1 Disable multi-threading of Intel MKL# - OPENBLAS_NUM_THREADS=1 Disable multi-threading of OpenBLASexport JAVA_HOME=/opt/java/jdk1.8.0_221export SCALA_HOME=/opt/scala/scala-2.12.10export HADOOP_HOME=/opt/hadoop/hadoop-3.1.2export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PYSPARK_PYTHON=/usr/local/bin/python3export PYSPARK_DRIVER_PYTHON=/usr/local/bin/ipython 启动在主节点启动master进程1start-master.sh 在备用主节点启动master进程1start-master.sh web浏览器访问 Web UI 启动全部worker节点在ALIVE MASTER节点 1start-slaves.sh unsplash-logoAustin Fruits","link":"/posts/735ce64f/"},{"title":"python配置","text":"记录 python 的一些基本常用配置，方便以后使用。 pip安装1$ sudo apt install python3-pip 升级1$ python -m pip install -U pip 错误处理报错1 pip has no attribute 'main'1module pip has no attribute 'main' 解决 1修改 /usr/bin/pip下 import pip 为 import pip._internal 报错2 1locale.Error: unsupported locale setting 解决 123$ export LC_ALL=\"en_US.UTF-8\"$ export LC_CTYPE=\"en_US.UTF-8\"$ sudo dpkg-reconfigure locales 换源1234567$ pip3 config set global.index-url https://mirrors.aliyun.com/pypi/simple/常用源豆瓣 https://pypi.doubanio.com/simple/网易 https://mirrors.163.com/pypi/simple/阿里云 https://mirrors.aliyun.com/pypi/simple/清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/ virtualenv &amp; virtualenvwrapper安装123$ pip3 install virtualenv$ pip3 install virtualenvwrapper # linux &amp; mac$ pip3 install virtualenvwrapper-win # win 添加环境变量linux&amp;mac下默认编辑.bashrc，若使用zsh，则编辑.zshrc，添加 1234export WORKON_HOME=$HOME/.virtualenvs # 虚拟环境的安装位置export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3 # 默认创建python的版本位置source /usr/local/bin/virtualenvwrapper.sh # virtualenvwrapper的位置，可用which命令查找(或者~/.local/bin/virtualenvwrapper.sh) 最后soucre文件生效 12$ source .bashrc &amp;$ source .zshrc win下直接添加环境变量 WORKON_HOME 常用的管理命令12345创建虚拟环境：mkvirtualenv new_env使用虚拟环境：workon new_env退出虚拟环境：deactivate删除虚拟环境: rmvirtualenv new_env查看所有虚拟环境：lsvirtualenv jupyter服务器远程访问配置生成配置文件1$ jupyter notebook --generate-config 生成/查看配置文件路径 生成密码打开ipython，创建一个密文的密码： 12345In [1]: from notebook.auth import passwdIn [2]: passwd()Enter password: Verify password: Out[2]: 'sha1:ce23d945972f:34769685a7ccd3d08c84a18c63968a41f1140274' 修改默认配置文件1$ vim ~/.jupyter/jupyter_notebook_config.py 进行如下修改： 1234c.NotebookApp.ip='*'c.NotebookApp.password = u'sha:ce...刚才复制的那个密文'c.NotebookApp.open_browser = Falsec.NotebookApp.port =8888 #随便指定一个端口 添加 kernel123456789# 在想要添加的 python 虚拟环境下pip3 install ipykernelpython -m ipykernel install --name kernelname# 查看所有 kerneljupyter kernelspec list# 删除 kerneljupyter kernelspec remove kernelname 后台运行12345678# 入门级jupyter notebook --allow-root &gt; jupyter.log 2&gt;&amp;1 &amp;# 进阶版nohup jupyter notebook --allow-root &gt; jupyter.log 2&gt;&amp;1 &amp;# 用 &amp; 让命令后台运行, 并把标准输出写入 jupyter.log 中# nohup 表示 no hang up, 就是不挂起, 于是这个命令执行后即使终端退出, 也不会停止运行. 终止进程 执行上面第 2 条命令, 可以发现关闭终端重新打开后, 用 jobs 找不到 jupyter 这个进程了, 于是要用 ps -a, 可以显示这个进程的 pid.kill -9 pid 终止进程 扩展插件1$ pip3 install jupyter_contrib_nbextensions 设置自动计算每个cell运行时间 12$ jupyter contrib nbextension install --user$ jupyter nbextension enable execute_time/ExecuteTime 更换主题1$ pip3 install jupyterthemes 查看可选主题 1$ jt-l 设置主题 1$ jt-t 其他奇技参考知乎回答 unsplash-logoZac Wolff","link":"/posts/8b7d2b6d/"},{"title":"markdown基本操作","text":"Markdown常用的一些语法集合。 数学公式及希腊字母行内与独行 行内公式：将公式插入到本行内，符号：$公式内容$，如：$xyz$ 独行公式：将公式插入到新的一行内，并且居中，符号：$$公式内容$$，如：$$xyz$$ 上标、下标与组合 上标符号，符号：^，如：$x^4$ 下标符号，符号：_，如：$x_1$ 组合符号，符号：{}，如：${16}{8}O{2+}{2}$ 汉字、字体与格式 汉字形式，符号：\\mbox{}，如：$V_{\\mbox{初始}}$ 字体控制，符号：\\displaystyle，如：$\\displaystyle \\frac{x+y}{y+z}$ 下划线符号，符号：\\underline，如：$\\underline{x+y}$ 标签，符号\\tag{数字}，如：$\\tag{11}$ 上大括号，符号：\\overbrace{算式}，如：$\\overbrace{a+b+c+d}^{2.0}$ 下大括号，符号：\\underbrace{算式}，如：$a+\\underbrace{b+c}_{1.0}+d$ 上位符号，符号：\\stacrel{上位符号}{基位符号}，如：$\\vec{x}\\stackrel{\\mathrm{def}}{=}{x_1,\\dots,x_n}$ 占位符 两个quad空格，符号：\\qquad，如：$x \\qquad y$ quad空格，符号：\\quad，如：$x \\quad y$ 大空格，符号\\，如：$x \\ y$ 中空格，符号\\:，如：$x : y$ 小空格，符号\\,，如：$x , y$ 没有空格，符号``，如：$xy$ 紧贴，符号\\!，如：$x ! y$ 定界符与组合 括号，符号：（）\\big(\\big) \\Big(\\Big) \\bigg(\\bigg) \\Bigg(\\Bigg)，如：$（）\\big(\\big) \\Big(\\Big) \\bigg(\\bigg) \\Bigg(\\Bigg)$ 中括号，符号：[]，如：$[x+y]$ 大括号，符号：\\{ \\}，如：${x+y}$ 自适应括号，符号：\\left \\right，如：$\\left(x\\right)$，$\\left(x{yz}\\right)$ 组合公式，符号：{上位公式 \\choose 下位公式}，如：${n+1 \\choose k}={n \\choose k}+{n \\choose k-1}$ 组合公式，符号：{上位公式 \\atop 下位公式}，如：$\\sum_{k_0,k_1,\\ldots&gt;0 \\atop k_0+k_1+\\cdots=n}A_{k_0}A_{k_1}\\cdots$ 四则运算 加法运算，符号：+，如：$x+y=z$ 减法运算，符号：-，如：$x-y=z$ 加减运算，符号：\\pm，如：$x \\pm y=z$ 减甲运算，符号：\\mp，如：$x \\mp y=z$ 乘法运算，符号：\\times，如：$x \\times y=z$ 点乘运算，符号：\\cdot，如：$x \\cdot y=z$ 星乘运算，符号：\\ast，如：$x \\ast y=z$ 除法运算，符号：\\div，如：$x \\div y=z$ 斜法运算，符号：/，如：$x/y=z$ 分式表示，符号：\\frac{分子}{分母}，如：$\\frac{x+y}{y+z}$ 分式表示，符号：{分子} \\voer {分母}，如：${x+y} \\over {y+z}$ 绝对值表示，符号：||，如：$|x+y|$ 高级运算 平均数运算，符号：\\overline{算式}，如：$\\overline{xyz}$ 开二次方运算，符号：\\sqrt，如：$\\sqrt x$ 开方运算，符号：\\sqrt[开方数]{被开方数}，如：$\\sqrt[3]{x+y}$ 对数运算，符号：\\log，如：$\\log(x)$ 极限运算，符号：\\lim，如：$\\lim^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$ 极限运算，符号：\\displaystyle \\lim，如：$\\displaystyle \\lim^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$ 求和运算，符号：\\sum，如：$\\sum^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$ 求和运算，符号：\\displaystyle \\sum，如：$\\displaystyle \\sum^{x \\to \\infty}_{y \\to 0}{\\frac{x}{y}}$ 积分运算，符号：\\int，如：$\\int^{\\infty}_{0}{xdx}$ 积分运算，符号：\\displaystyle \\int，如：$\\displaystyle \\int^{\\infty}_{0}{xdx}$ 微分运算，符号：\\partial，如：$\\frac{\\partial x}{\\partial y}$ 矩阵表示，符号：\\begin{matrix} \\end{matrix}，如：$\\left[ \\begin{matrix} 1 &amp;2 &amp;\\cdots &amp;4\\5 &amp;6 &amp;\\cdots &amp;8\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\13 &amp;14 &amp;\\cdots &amp;16\\end{matrix} \\right]$ 逻辑运算 等于运算，符号：=，如：$x+y=z$ 大于运算，符号：&gt;，如：$x+y&gt;z$ 小于运算，符号：&lt;，如：$x+y&lt;z$ 大于等于运算，符号：\\geq，如：$x+y \\geq z$ 小于等于运算，符号：\\leq，如：$x+y \\leq z$ 不等于运算，符号：\\neq，如：$x+y \\neq z$ 不大于等于运算，符号：\\ngeq，如：$x+y \\ngeq z$ 不大于等于运算，符号：\\not\\geq，如：$x+y \\not\\geq z$ 不小于等于运算，符号：\\nleq，如：$x+y \\nleq z$ 不小于等于运算，符号：\\not\\leq，如：$x+y \\not\\leq z$ 约等于运算，符号：\\approx，如：$x+y \\approx z$ 恒定等于运算，符号：\\equiv，如：$x+y \\equiv z$ 集合运算 属于运算，符号：\\in，如：$x \\in y$ 不属于运算，符号：\\notin，如：$x \\notin y$ 不属于运算，符号：\\not\\in，如：$x \\not\\in y$ 子集运算，符号：\\subset，如：$x \\subset y$ 子集运算，符号：\\supset，如：$x \\supset y$ 真子集运算，符号：\\subseteq，如：$x \\subseteq y$ 非真子集运算，符号：\\subsetneq，如：$x \\subsetneq y$ 真子集运算，符号：\\supseteq，如：$x \\supseteq y$ 非真子集运算，符号：\\supsetneq，如：$x \\supsetneq y$ 非子集运算，符号：\\not\\subset，如：$x \\not\\subset y$ 非子集运算，符号：\\not\\supset，如：$x \\not\\supset y$ 并集运算，符号：\\cup，如：$x \\cup y$ 交集运算，符号：\\cap，如：$x \\cap y$ 差集运算，符号：\\setminus，如：$x \\setminus y$ 同或运算，符号：\\bigodot，如：$x \\bigodot y$ 同与运算，符号：\\bigotimes，如：$x \\bigotimes y$ 实数集合，符号：\\mathbb{R}，如：\\mathbb{R} 自然数集合，符号：\\mathbb{Z}，如：\\mathbb{Z} 空集，符号：\\emptyset，如：$\\emptyset$ 数学符号 无穷，符号：\\infty，如：$\\infty$ 虚数，符号：\\imath，如：$\\imath$ 虚数，符号：\\jmath，如：$\\jmath$ 数学符号，符号\\hat{a}，如：$\\hat{a}$ 数学符号，符号\\check{a}，如：$\\check{a}$ 数学符号，符号\\breve{a}，如：$\\breve{a}$ 数学符号，符号\\tilde{a}，如：$\\tilde{a}$ 数学符号，符号\\bar{a}，如：$\\bar{a}$ 矢量符号，符号\\vec{a}，如：$\\vec{a}$ 数学符号，符号\\acute{a}，如：$\\acute{a}$ 数学符号，符号\\grave{a}，如：$\\grave{a}$ 数学符号，符号\\mathring{a}，如：$\\mathring{a}$ 一阶导数符号，符号\\dot{a}，如：$\\dot{a}$ 二阶导数符号，符号\\ddot{a}，如：$\\ddot{a}$ 上箭头，符号：\\uparrow，如：$\\uparrow$ 上箭头，符号：\\Uparrow，如：$\\Uparrow$ 下箭头，符号：\\downarrow，如：$\\downarrow$ 下箭头，符号：\\Downarrow，如：$\\Downarrow$ 左箭头，符号：\\leftarrow，如：$\\leftarrow$ 左箭头，符号：\\Leftarrow，如：$\\Leftarrow$ 右箭头，符号：\\rightarrow，如：$\\rightarrow$ 右箭头，符号：\\Rightarrow，如：$\\Rightarrow$ 底端对齐的省略号，符号：\\ldots，如：$1,2,\\ldots,n$ 中线对齐的省略号，符号：\\cdots，如：$x_1^2 + x_2^2 + \\cdots + x_n^2$ 竖直对齐的省略号，符号：\\vdots，如：$\\vdots$ 斜对齐的省略号，符号：\\ddots，如：$\\ddots$ 希腊字母 字母 实现 字母 实现 A A α \\alhpa B B β \\beta Γ \\Gamma γ \\gamma Δ \\Delta δ \\delta E E ϵ \\epsilon Z Z ζ \\zeta H H η \\eta Θ \\Theta θ \\theta I I ι \\iota K K κ \\kappa Λ \\Lambda λ \\lambda M M μ \\mu N N ν \\nu Ξ \\Xi ξ \\xi O O ο \\omicron Π \\Pi π \\pi P P ρ \\rho Σ \\Sigma σ \\sigma T T τ \\tau Υ \\Upsilon υ \\upsilon Φ \\Phi ϕ \\phi X X χ \\chi Ψ \\Psi ψ \\psi Ω \\v ω \\omega 流程图横向流程图源码格式:123456graph LRA[方形] --&gt; B(圆角) B --&gt; C{条件a} C --&gt; |a=1| D[结果1] C --&gt; |a=2| E[结果2] F[横向流程图] 123456graph LRA[方形] --&gt; B(圆角) B --&gt; C{条件a} C --&gt; |a=1| D[结果1] C --&gt; |a=2| E[结果2] F[横向流程图] 竖向流程图源码格式:123456graph TDA[方形] --&gt; B(圆角) B --&gt; C{条件a} C --&gt; |a=1| D[结果1] C --&gt; |a=2| E[结果2] F[竖向流程图] 123456graph TDA[方形] --&gt; B(圆角) B --&gt; C{条件a} C --&gt; |a=1| D[结果1] C --&gt; |a=2| E[结果2] F[竖向流程图] 标准流程图源码格式:123456graph TDA[方形] --&gt; B(圆角) B --&gt; C{条件a} C --&gt; |a=1| D[结果1] C --&gt; |a=2| E[结果2] F[竖向流程图] 123456789st=&gt;start: 开始框op=&gt;operation: 处理框cond=&gt;condition: 判断框sub1=&gt;subroutine: 子流程io=&gt;inputoutput: 输入输出框e=&gt;end: 结束框st-&gt;op-&gt;condcond(yes)-&gt;io-&gt;econd(no)-&gt;sub1(right)-&gt;op 标准流程图源码格式(横向):123456789st=&gt;start: 开始框op=&gt;operation: 处理框cond=&gt;condition: 判断框(是或否?)sub1=&gt;subroutine: 子流程io=&gt;inputoutput: 输入输出框e=&gt;end: 结束框st(right)-&gt;op(right)-&gt;condcond(yes)-&gt;io(bottom)-&gt;econd(no)-&gt;sub1(right)-&gt;op 123456789st=&gt;start: 开始框op=&gt;operation: 处理框cond=&gt;condition: 判断框(是或否?)sub1=&gt;subroutine: 子流程io=&gt;inputoutput: 输入输出框e=&gt;end: 结束框st(right)-&gt;op(right)-&gt;condcond(yes)-&gt;io(bottom)-&gt;econd(no)-&gt;sub1(right)-&gt;op UML时序图源码样例:12345对象A-&gt;对象B: 对象B你好吗? (请求)Note right of 对象B: 对象B的描述Note left of 对象A: 对象A的描述(提示)对象B --&gt; 对象A: 我很好(响应)对象A --&gt; 对象B: 你真的好吗? 12345对象A-&gt;对象B: 对象B你好吗? (请求)Note right of 对象B: 对象B的描述Note left of 对象A: 对象A的描述(提示)对象B --&gt; 对象A: 我很好(响应)对象A --&gt; 对象B: 你真的好吗? UML时序图源码复杂样例:1234567891011Title: 标题: 复杂使用对象A -&gt; 对象B: 对象B你好吗? (请求)Note right of 对象B: 对象B的描述Note right of 对象A: 对象A的描述(提示)对象B --&gt; 对象A: 我很好(响应)对象B --&gt; 小三: 你好吗?小三 -&gt; 对象A: 对象B找我了对象A --&gt; 对象B: 你真的好吗?Note over 小三, 对象B: 我们是朋友participant CNote right of C: 没人陪我玩 1234567891011Title: 标题: 复杂使用对象A -&gt; 对象B: 对象B你好吗? (请求)Note right of 对象B: 对象B的描述Note right of 对象A: 对象A的描述(提示)对象B --&gt; 对象A: 我很好(响应)对象B --&gt; 小三: 你好吗?小三 -&gt; 对象A: 对象B找我了对象A --&gt; 对象B: 你真的好吗?Note over 小三, 对象B: 我们是朋友participant CNote right of C: 没人陪我玩 UML标准时序图样例:123456789101112%%时序图例子, -&gt; 实线, --&gt; 虚线, -&gt;&gt; 实线箭头 sequenceDiagram participant 张三 participant 李四 张三 -&gt; 王五: 王五你好吗? loop 健康检查 王五 -&gt; 王五: 与疾病战斗 end Note right of 王五: 合理饮食 &lt;br/&gt;看医生... 李四 -&gt;&gt; 张三: 很好! 王五 -&gt; 李四: 你怎么样? 李四 --&gt; 王五: 很好! 123456789101112%%时序图例子, -&gt; 实线, --&gt; 虚线, -&gt;&gt; 实线箭头 sequenceDiagram participant 张三 participant 李四 张三 -&gt; 王五: 王五你好吗? loop 健康检查 王五 -&gt; 王五: 与疾病战斗 end Note right of 王五: 合理饮食 &lt;br/&gt;看医生... 李四 -&gt;&gt; 张三: 很好! 王五 -&gt; 李四: 你怎么样? 李四 --&gt; 王五: 很好! 甘特图样例:12345678910111213141516171819202122%%语法示例 gantt dateFormat YYYY-MM-DD title 软件开发甘特图 section 设计 需求 :done, des1, 2014-01-06, 2014-01-08 原型 :active, des2, 2014-01-09, 3d UI设计 :des3, after des2, 5d 未来任务: :des4, after des3, 5d section 开发 学习准备理解需求 :crit, done, 2014-01-06, 24h 设计框架 :crit, done, after des2, 2d 开发 :crit, active, 3d 未来任务 :crit, 5d 耍 :2d section 测试 功能测试 :active, a1, after des3, 3d 压力测试 :after a1, 20h 测试报告 :48h 12345678910111213141516171819202122%%语法示例 gantt dateFormat YYYY-MM-DD title 软件开发甘特图 section 设计 需求 :done, des1, 2014-01-06, 2014-01-08 原型 :active, des2, 2014-01-09, 3d UI设计 :des3, after des2, 5d 未来任务: :des4, after des3, 5d section 开发 学习准备理解需求 :crit, done, 2014-01-06, 24h 设计框架 :crit, done, after des2, 2d 开发 :crit, active, 3d 未来任务 :crit, 5d 耍 :2d section 测试 功能测试 :active, a1, after des3, 3d 压力测试 :after a1, 20h 测试报告 :48h typora 快捷键 unsplash-logoKevin Mueller","link":"/posts/5df87593/"},{"title":"spark基本概念","text":"RDD Programming Guide翻译自官方文档，本文值翻译了部分 Python 部分。 概述从高层次看，每个 Spark 应用都包含一个驱动程序，用于执行用户的 main 函数以及在集群上进行各种并行操作。Spark 提供的重要抽象是弹性分布式数据集（ resilient distributed dataset），这是一个包含诸多元素、被划分到不同节点上进行并行处理的数据集。RDDs 通过打开一个 HDFS 文件（或者其他任何 hadoop 支持的文件系统）、在驱动程序中打开一个已有的 Scala 数据集或者由其他 RDD 转换得到。用户可以要求 Spark 将 RDD 持久化到内存中，这样可以有效的在并行运算中复用。另外，RDDs 在节点发生错误时会自动恢复。 Spark 的另一个抽象是在并行运算中使用的共享变量。在默认情况下，当 Spark 将一个函数转化成许多任务在不同节点上运行的时候，对于所有在函数中使用的变量，每一个任务都会得到一个副本。有时，一个变量需要在任务之间或者任务和驱动程序之间共享。Spark 支持两种共享变量：广播变量，用来将一个值缓存到所有节点的内存中；累加器，只能用于累加，比如计数器和求和。 连接 SparkSpark2.4.4 支持 Python2.7+ 或 Python3.4+。它使用标准的 CPython 解释器，因此诸如 NumPy 之类的 C 库也是可以使用的。 Python 中的 Spark 应用程序既可以在运行时使用包含 Spark 的 bin/spark-submit 脚本运行，也可以将其包含在setup.py 中，如下所示： 123install_requires=[ 'pyspark=={site.SPARK_VERSION}' ] 要在 Python 中运行 Spark 应用程序而无需安装 PySpark，请使用 Spark 目录中的 bin/spark-submit 脚本。 该脚本将加载 Spark 的 Java/Scala 库，并允许你将应用程序提交到集群。 你还可以使用 bin/pyspark启动交互是 Python Shell。 如果你想要访问 HDFS 中的数据，你需要为你使用的 HDFS 版本建立一个 PySpark 连接。 最后，你需要将一些 Spark 类 import 到你的程序中。加入如下这行： 1from pyspark import SparkContext, SparkConf 初始化 Spark在一个 Spark 程序中要做的第一件事就是创建一个 SparkContext 对象来高速 Spark 如何连接一个集群。要创建一个 SparkContext，你首先要创建一个包含你的应用的信息的 SparkConf 对象。 12conf = SparkConf().setAppName(appName).setMaster(master)sc = SparkContext(conf=conf) appName 参数是在集群 UI 上显示你的应用名称。master 是一个 Spark、Mesos 或 YARN 集群的 URL，特殊的 “local” 字符串表示在本地运行。在实际使用中，当你在集群中运行你的程序，你一般不会把 master 参数在代码中写死，而是通过 spark-submit 运行程序来回去这个参数。但是，在本地测试及单元测试时，你仍需要传入 “local” 参数来运行程序。 使用命令行在 PySpark 命令行中，一个特殊的集成在解释器里的 SparkContext 变量已经建立好了，变量名叫做 sc 。创建你自己的 SparkContext 不会起作用。你可以通过使用 —master 命令行参数来设置这个上下文连接的 master 主机，你也可以通过 —py-files 参数传递一个用逗号隔开的列表来将 Python 的 .zip、.egg 或. py 文件添加到运行时路径中。你还可以通过 —package 参数传递一个用逗号隔开的 maven 列表来给这个命令行会话添加依赖（比如Spark 的包）。任何额外的包含依赖包的仓库（比如 SonaType ）都可以通过传给 —repositorys 参数来添加进去。Spark 包的所有 Python 依赖（列在这个包的 requirements.txt 文件中）在必要时都必须通过 pip 手动安装。 比如，使用四核来运行bin/pyspark应当输入这个命令： 1$ ./bin/pyspark --master local[4] 又比如，把 code.py 文件添加到搜索路径中（为了能够 import 在程序中），应当使用这条命令： 1$ ./bin/pyspark --master local[4] --py-files code.py 想要了解命令行选项的完整信息请执行 pyspark –help 命令。在这些场景下，pyspark 会触发一个更通用的 spark-submit 脚本 在 IPython 这个加强的 Python 解释器中运行 PySpark 也是可行的。PySpark 可以在 1.0.0 或更高版本的 IPython上运行。为了使用 IPython，必须在运行 bin/pyspark 时将 PYSPARK_DRIVER_PYTHON 变量设置为 ipython，就像这样： 1$ PYSPARK_DRIVER_PYTHON=ipython ./bin/pyspark 你还可以通过设置 PYSPARK_DRIVER_PYTHON_OPTS 来定制化 ipython 或 jupyter。 弹性分布式数据集（RDD）Spark 是以 RDD概念为中心运行的。RDD 是一个容错的、可以被并行运算的元素集合。有两种方法创建 RDDs：在你的驱动程序中并行化一个已经存在的集合；从外部存储系统中引用一个数据集，例如一个共享文件系统、HDFS、HBase 或任何提供了 Hadoop 输入格式的数据源。 并行化数据集并行化数据集是通过在驱动程序中一个现有的迭代器或集合上调用 SparkContext 的 parallelize 方法建立的。为了创建一个能够并行操作的分布数据集，集合中的元素都会被拷贝。比如，以下的语句建立了一个包含 1 到 5 的并行化数据集： 12data = [1, 2, 3, 4, 5]distData = sc.parallelize(data) 分布式数据集一旦被建立，就可以进行并行运算。例如，我们可以调用 distData.reduce(lambda a, b: a + b) 来对元素进行累加。在后文我们再详细描述分布数据集上的操作。 并行集合的一个重要参数是将数据集划分成片的数量。对于每一个分片，Spark 会再集群中运行一个对应的任务。典型情况下，集群中的每个 CPU 将对应运行 2-4 个分片。通常，Spark 会根据你的集群自动设置分片的数量。但是，你也可以通过将第二个参数传递给 parallelize 方法(比如 sc.parallelize(data, 10))来手动确定分片数量。注意：有些代码中会使用切片（slice，分片的同义词）这个术语来保持向下兼容性。 外部数据集PySpark 可以通过 Hadoop 支持的外部数据源（包括本地文件系统、HDFS、Cassandra、HBase、Amazon S3 等等）创建分布数据集。Spark 支持文本文件、序列文件和其他任何 Hadoop 输入格式的文件。 创建文本文件 RDDs 要使用 SparkContext 的 textFile 方法。这个方法输入一个文件的 URL （本地文件路径或 hdfs://、s3a://等等的 URl）然后读入这个文件并建立一个文本行集合。以下是一个例子： 1distFile = sc.textFile(\"data.txt\") 建立完成后 distFIle 就可以调用数据集操作了。例如，我们可以调用 map 和 reduce 操作来累加所有文件行的长度，代码如下： 1distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b) 在 Spark 读入文件时有几点要注意： 如果使用本地文件路径，要保证文件在工作节点上这个文件也能通过相同的路径访问到。这点可以通过复制文件到所有节点或使用网络挂载的共享文件系统解决。 包括 textFile 在内的所有基于 Spark 文件读入方法都支持将文件夹、压缩文件和通配符的路径作为参数。比如，以下表达都是合法的： 123textFile(\"/my/directory\")textFile(\"/my/directory/*.txt\")textFile(\"/my/directory/*.gz\") textFile 也可以通过传入第二个可选参数来控制文件的分片数量。默认情况下，Spark 为文件的每个块创建一个分片（HDFS 中默认块的大小为 128MB）。但是你也可以通过传入一个更大的值来要求建立更多的分片。注意，分片的数量不能比文件块的数量少。 除了文本文件，Spark 的 Python API 还支持多种其他数据格式： SparkContext.wholeTextFiles 能够读入包含多个小文件的目录，然后为每个文件返回一个 (filename, content) 对。这是与 textFIle 为每个文本行返回一条记录想对应。 RDD.saveAsPickleFile 和 SparkContext.pickleFile 方法支持将 RDD 以简单序列化的 Python 对象格式保存。序列化的过程中会以默认10个一批的数量批量处理。 序列文件和其他 Hadoop 输入输出文件。 注意 这个特性目前仍处于试验阶段，被标记为Experimental，目前只适用于高级用户。这个特性在未来可能会被基于Spark SQL的读写支持所取代，因为Spark SQL是更好的方式。 可写类型支持PySpark 序列文件支持利用 Java 作为中介载入一个键值对 RDD，将可写类型转化成 Java 的基本类型，然后使用 Pyrolite 将 java 结果对象序列化。当将一个键值对 RDD 储存到一个序列文件中时 PySpark 将会运行上述过程的相反过程。首先将 Python 对象反序列化成 Java 对象，然后转化成可写类型。以下可写类型会自动转换： Writable Type Python Type Text unicode str IntWritable int FloatWritable float DoubleWritable float BooleanWritable bool BytesWritable bytearray NullWritable None MapWritable dict 数组不能自动转换。用户需要在读写时指定 ArrayWritable 的子类型。在读入的时候，默认的转换器会把自定义的 ArrayWritable 子类型转化成 Java 的 Object[]，之后序列化成 Python 的元组。为了获得 Python 的 array.array 类型来使用主要类型的数组，用户需要自行指定转换器。 保存和读取序列文件和文本文件类似，序列文件可以通过指定的路径来保存与读取。键值的类型可以自定义，但对于标准的可写类型可以不指定： 1234&gt;&gt;&gt; rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))&gt;&gt;&gt; rdd.saveAsSequenceFile(\"path/to/file\")&gt;&gt;&gt; sorted(sc.sequenceFile(\"path/to/file\").collect())[(1, u'a'), (2, u'aa'), (3, u'aaa')] 保存和读取其他 Hadoop 输入输出格式PySpark 也可以读取和写入其他 Hadoop 输入输出格式，包括新旧两种 Hadoop MapReduce APIs。如果有必要，一个 Hadoop 配置文件也可以以 Python 字典的形式传入。以下是一个使用 Elasticsearch ESInputFormat 的例子： 12345678910&gt;&gt;&gt; conf = {\"es.resource\" : \"index/type\"} # assume Elasticsearch is running on localhost defaults&gt;&gt;&gt; rdd = sc.newAPIHadoopRDD(\"org.elasticsearch.hadoop.mr.EsInputFormat\", \"org.apache.hadoop.io.NullWritable\", \"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=conf)&gt;&gt;&gt; rdd.first() # the result is a MapWritable that is converted to a Python dict(u'Elasticsearch ID', {u'field1': True, u'field2': u'Some Text', u'field3': 12345}) 注意，如果这个读入格式仅仅依赖于一个 Hadoop 配置和/或输入路径，而且键值类型都可以根据前面的表格直接转换，那么刚才提到的这种方法非常合适。 如果你有一些自定义的序列化二进制数据（比如从 Cassandra/HBase 中读取数据），那么你需要首先在 Scala/Java 端将这些数据转化成可以被 Pyrolite 的串行化器处理的数据类型。一个转换器特质已经提供好了。简单地拓展这个特质同时在convert方法中实现你自己的转换代码即可。记住，要确保这个类以及访问你的输入格式所需的依赖都被打到了 Spark 作业包中，并且确保这个包已经包含到了 PySpark 的 classpath 中。 这里有一些通过自定义转换器来使用 Cassandra/HBase 输入输出格式的Python样例和转换器样例。 RDD 操作RDD支持两种类型的操作：转换（从现有操作创建新的数据集）和动作（在操作数据集上进行计算后，将值返回给驱动程序）。例如，map 是一个转换，它将每个数据集元素通过一个函数传递，并返回代表结果的新 RDD。另一方面，reduce 是使用某些函数聚合 RDD 的所有元素并将最终结果返回给驱动程序的操作（尽管也有并行的 reduceByKey 返回分布式数据集）。 Spark 中的所有转换都是惰性的，因为它们不会立即计算出结果。取而代之的是，他们只记得应用于某些基本数据集（例如文件）的转换。仅当动作要求将结果返回给驱动程序时才计算转换。这种设计使 Spark 可以更高效地运行。例如，我们可以认识到通过 map 创建的数据集将用于 reduce 中，并且仅将 reduce 的结果返回给驱动程序，而不是将较大的 maped 数据集返回给驱动程序。 默认情况下，每次在其上执行操作时，可能都会重新计算每个转换后的 RDD。但是，您也可以使用 persist（或缓存）方法将 RDD 保留在内存中，在这种情况下，Spark 会将元素保留在群集中，以便下次查询时可以更快地进行访问。还支持将 RDD 持久存储在磁盘上，或在多个节点之间复制。 基本操作为了说明 RDD 的基本操作，请看以下的简单程序： 123lines = sc.textFile(\"data.txt\")lineLengths = lines.map(lambda s: len(s))totalLength = lineLengths.reduce(lambda a, b: a + b) 第一行定义了一个由外部文件产生的基本 RDD。这个数据集不是从内存中载入的也不是由其他操作产生的；lines 仅仅是一个指向文件的指针。第二行将 lineLengths 定义为 map 操作的结果。再强调一次，由于惰性求值的缘故，lineLengths 并不会被立即计算得到。最后，我们运行了 reduce 操作，这是一个启动操作。从这个操作开始，Spark 将计算过程划分成许多任务并在多机上运行，每台机器运行自己部分的 map 操作和 reduce 操作，最终将自己部分的运算结果返回给驱动程序。 如果w筽们虚妄以后重复使用 lineLengths，只需要在 reduce 前加入下面这行代码： 1lineLengths.persist() 这条代码将使得 lineLengths 在第一次计算生成之后保存在内存中。 向 Spark 传递函数Spark 的 API 严重依赖于向驱动程序传递函数作为参数来在集群上运行。有三种推荐的方法来传递函数作为参数： Lambda 表达式，简单的函数可以直接写成一个 lambda 表达式（lambda 表达式不支持多语句函数和无返回值的语句）。 对于代码很长的函数，在 Spark 的函数调用中定义。 模块中的顶层函数。 例如，传递一个无法转化为 lambda 表达式的长函数，可以像下面代码这样： 12345678\"\"\"MyScript.py\"\"\"if __name__ == \"__main__\": def myFunc(s): words = s.split(\" \") return len(words) sc = SparkContext(...) sc.textFile(\"file.txt\").map(myFunc) 值得指出的是，也可以传递类实例中方法的引用（与单例对象相反），这种传递方法会将整个对象传递过去。比如，考虑以下代码： 12345class MyClass(object): def func(self, s): return s def doStuff(self, rdd): return rdd.map(self.func) 在这里，如果我们创建了一个新的 MyClass 对象，然后对它调用 doStuff 方法，map 会用到这个对象中方法的引用，所以整个对象都需要传递到集群中。 还有另一种相似的写法，访问外层对象的数据域会传递整个对象的引用： 12345class MyClass(object): def __init__(self): self.field = \"Hello\" def doStuff(self, rdd): return rdd.map(lambda s: self.field + x) 此类问题最简单的避免方法就是，使用一个本地变量缓存一份这个数据域的拷贝，直接访问这个数据域： 123def doStuff(self, rdd): field = self.field return rdd.map(lambda s: field + x) 理解闭包关于 Spark 的难点之一是理解在跨集群执行代码时变量和方法的作用域和生命周期。修改超出其作用域的 RDD 操作可能经常引起混乱。在下面的示例中，我们将介绍使用 foreach() 递增计数器的代码，其他操作也会导致类似的问题。 示例考虑以下朴素的 RDD 元素求和，其结果可能会有所不同，具体取决于是否在同一 JVM 中进行执行。 一个常见的例子是在本地模式下运行 Spark（–master = local [n]）而不是将 Spark 应用程序部署到集群上（例如，通过将 spark-submit 提交给 YARN）： 12345678910counter = 0rdd = sc.parallelize(data)# Wrong: Don't do this!!def increment_counter(x): global counter counter += xrdd.foreach(increment_counter)print(\"Counter value: \", counter) 本地 vs 集群模式上面的代码的行为是未定义的，可能无法按预期工作。为了执行作业，Spark 将 RDD 操作的处理分解为小任务，每个任务都由执行程序执行。在执行之前，Spark 会计算任务的结束时间。闭包是执行者在 RDD 上执行其计算所必须可见的那些变量和方法（在本例中为foreach（））。此闭包被序列化并发送给每个执行器。 发送给每个执行者的闭包中的变量现在都是副本，因此，在 foreach 函数中引用计数器时，它不再是驱动程序节点上的计数器。驱动程序节点的内存中仍然存在一个计数器，但是执行者将不再看到该计数器！执行者仅从序列化闭包中看到副本。因此，由于对计数器的所有操作都引用了序列化闭包内的值，所以计数器的最终值仍将为零。 在本地模式下，在某些情况下，foreach 函数实际上将在与驱动程序相同的 JVM 中执行，并且将引用相同的原始计数器，并且可能会对其进行实际更新。 为了确保在此类情况下行为明确，应使用累加器。 Spark 中的累加器专门用于提供一种机制，用于在集群中的各个工作节点之间拆分执行时安全地更新变量。本指南的“累加器”部分将详细讨论这些内容。 通常，闭包——像循环或局部定义的方法之类的结构，不应用于改变某些全局状态。 Spark 不定义或保证从闭包外部引用的对象的突变行为。某些执行此操作的代码可能会在本地模式下工作，但这只是偶然的情况，此类代码在分布式模式下将无法正常运行。如果需要一些全局聚合，请使用累加器。 打印 RDD 的元素另一个常见用法是尝试使用 rdd.foreach(println) 或 rdd.map(println) 打印出 RDD 的元素。 在单台机器上，这将生成预期的输出并打印所有 RDD 的元素。 但是，在集群模式下，执行者正在调用 stdout 的输出现在写入执行者的 stdout，而不是驱动程序上的那个，因此驱动程序上的 stdout 不会显示这些信息！ 要在驱动程序上打印所有元素，可以使用 collect() 方法先将 RDD 带到驱动程序节点：rdd.collect().foreach(println)。 但是，这可能会导致驱动程序用尽内存，因为 collect() 将整个 RDD 提取到一台计算机上。 如果只需要打印 RDD 的一些元素，更安全的方法是使用 take()：rdd.take(100).foreach(println)。 使用键值对尽管大多数 Spark 操作可在包含任何类型的对象的 RDD 上运行，但一些特殊操作仅可用于键-值对的 RDD。 这类操作中最常见的是分布式 shuffle 操作，例如通过键对元素进行分组或聚合。 在 Python中，这些操作适用于包含内置 Python 元组（例如（1、2））的 RDD。 只需创建这样的元组，然后调用所需的操作即可。 例如，以下代码对键值对使用 reduceByKey 操作来统计文件中每一行文本出现的次数： 123lines = sc.textFile(\"data.txt\")pairs = lines.map(lambda s: (s, 1))counts = pairs.reduceByKey(lambda a, b: a + b) 例如，我们还可以使用 counts.sortByKey() 对字母对进行排序，最后使用 counts.collect() 将它们作为对象列表返回。 Transformations 操作下面的表格列出了 Spark 支持的常用 Transformations 操作。详细细节，请查阅 RDD API文档（Scala, Java, Python, R）和键值对 RDD 函数文档 （Scala, Java）。 （注：次部分翻译比较简略，仅供参考，具体细节请看文档） Transformation Meaning map(func) 返回一个新的分布数据集，由原数据集元素经 func 处理后的结果组成 filter(func) 返回一个新的数据集，由传给func 返回 True 的原数据集元素组成 flatMap(func) 与 map 类似，但是每个传入元素可能有 0 或多个返回值，func 可以返回一个序列而不是一个值 mapPartitions(func) 类似 map，但是 RDD 的每个分片都会分开独立运行，所以 func 的参数和返回值必须都是迭代器 mapPartitionsWithIndex(func) 类似 mapParitions，但是 func 有两个参数，第一个是分片的序号，第二个是迭代器。返回值还是迭代器 sample(withReplacement, fraction, seed) 使用提供的随机数种子取样，然后替换或不替换 union(otherDataset) 返回新的数据集，包括原数据集和参数数据集的所有元素 intersection(otherDataset) 返回新数据集，是两个集的交集 distinct([numPartitions])) 返回新的集，包括原集中的不重复元素返回新的集，包括原集中的不重复元素 groupByKey([numPartitions]) 当用于键值对 RDD 时返回 (键，值迭代器) 对的数据集 reduceByKey(func, [numPartitions]) 当用于键值对 RDD 时返回 (键，值迭代器) 对的数据集。其中每个键的值使用给定的 reduce 函数进行汇总 aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions]) 用于键值对 RDD 时返回（K，U）对集，对每一个 Key 的 value进行聚集计算 sortByKey([ascending], [numPartitions]) 用于键值对 RDD 时会返回 RDD 按键的顺序排序，升降序由第一个参数决定 join(otherDataset, [numPartitions]) 用于键值对 (K, V) 和 (K, W) RDD时返回 (K, (V, W)) 对 RDD cogroup(otherDataset, [numPartitions]) 用于两个键值对RDD时返回 (K, (V 迭代器， W 迭代器)) RDD cartesian(otherDataset) 用于 T 和 U 类型 RDD 时返回 (T, U) 对类型键值对 RDD pipe(command, [envVars]) 通过 shell 命令管道处理每个 RDD 分片 coalesce(numPartitions) 把 RDD 的分片数量降低到参数大小 repartition(numPartitions) 重新打乱 RDD 中元素顺序并重新分片，数量由参数决定 repartitionAndSortWithinPartitions(partitioner) 按照参数给定的分片器重新分片，同时每个分片内部按照键排序 Actions 操作下面的表格列出了 Spark 支持的常用 Actions 操作。详细细节，请查阅 RDD API文档 （Scala, Java, Python, R）和键值对 RDD 函数文档 （Scala, Java）。 （注：次部分翻译比较简略，仅供参考，具体细节请看文档） Action Meaning reduce(func) 使用 func 进行聚合计算，func 的参数是两个，返回值一个，两次 func 运行应当是完全解耦的，这样才能正确地并行运算 collect() 向驱动程序返回数据集的元素组成的数组 count() 返回数据集元素的数量 first() 返回数据集的第一个元素 take(n) 返回前n个元素组成的数组 takeSample(withReplacement, num, [seed]) 返回一个由原数据集中任意 num 个元素的数组，并且替换之 takeOrdered(n, [ordering]) 返回排序后的前 n 个元素 saveAsTextFile(path) 将数据集的元素写成文本文件 saveAsSequenceFile(path) (Java and Scala) 数据集的元素写成序列文件，这个 API 只能用于 Java 和 Scala 程序 saveAsObjectFile(path) (Java and Scala) 将数据集的元素使用 Java 的序列化特性写到文件中，这个 API 只能用于 Java 和 Scala 程序 countByKey() 只能用于键值对 RDD，返回一个 (K, int) hashmap，返回每个 key 的出现次数 foreach(func) 对数据集的每个元素执行 func, 通常用于完成一些带有副作用的函数，比如更新累加器（见下文）或与外部存储交互等 Spark RDD API 还展示了某些操作的异步版本，例如 foreachAsync for foreach，它们立即将FutureAction 返回给调用方，而不是在操作完成时阻塞。 这可用于管理或等待动作的异步执行。 Shuffle 操作Spark 中的某些操作会触发一个称为 shuffle 的事件。 shuffle 是 Spark 的一种用于重新分配数据的机制，可以跨分区对数据进行不同的分组。 这通常涉及跨执行程序和机器复制数据，从而使得 shuffle 成为复杂且高代价的操作。 背景要了解 shuffle 期间发生的情况，我们可以考虑 reduceByKey 操作的示例。 reduceByKey 操作会生成一个新的 RDD，其中将单个键的所有值组合为一个元组——该键以及针对与该键关联的所有值执行 reduce 函数的结果。难点在于，并非每个键的所有值都必须位于同一分区，甚至同一台计算机上，但是必须将它们放在同一位置才能计算结果。 在 Spark 中，数据通常不会跨分区分布在特定操作的必要位置。在计算期间，单个任务将在单个分区上进行操作-因此，为了组织要执行的单个 reduceByKey reduce 任务的所有数据，Spark 需要执行所有操作。它必须从所有分区读取以找到所有键的所有值，然后将跨分区的值汇总在一起以计算每个键的最终结果——这称为 shuffle。 尽管新改组后的数据的每个分区中的元素集都是确定性的，分区本身的顺序也是如此，但这些元素的顺序不是确定性的。如果你希望在 shuffle 后能使数据有序，则可以使用： mapPartitions 使用例如. sorted 对每个分区进行排序 repartitionAndSortWithinPartitions 可以有效地对分区进行排序，同时进行重新分区 排序以生成全局排序的 RDD 可能导致 shuffle 的操作包括重新分区操作（ repartition 和 coalesce），ByKey操作（除计数）（例如 groupByKey 和 reduceByKey）以及联接操作（例如 cogroup 和 join）。 性能影响shuffle 是一项高代价的操作，因为它涉及磁盘 I/O，数据序列化和网络 I/O。为了组织随机数据，Spark 生成任务集——map 任务以组织数据，以及一组 reduce 任务来聚合数据。此术语来自 MapReduce，与 Spark 的 map 和reduce 操作没有直接关系。 在内部，单个 map 任务的结果会保留在内存中，直到无法容纳为止。然后，根据目标分区对它们进行排序并写入单个文件。在 reduce 方面，任务读取相关的已排序块。 某些 shuffle 操作会占用大量的堆内存，因为它们在转移它们之前或之后采用内存中的数据结构来组织记录。具体来说，reduceByKey 和a ggregateByKey 在 map 侧创建这些结构，而 ByKey 操作在 reduce 侧生成这些结构。当内存存不下数据是时，Spark 会将这些表溢出到磁盘上，从而产生磁盘 I/O 的额外开销并增加垃圾回收。 shuffle 还会在磁盘上生成大量中间文件。从 Spark 1.3 开始，将保留这些文件直到不再使用相应的 RDD 并进行垃圾回收为止。这样做是为了在重新计算沿袭时无需重新创建 shuffle 文件。如果应用程序保留了对这些 RDD 的引用，或者如果 GC 不经常启动，则垃圾回收可能仅在很长一段时间后才会发生。这意味着长时间运行的 Spark 作业可能会占用大量磁盘空间。在配置 Spark 上下文时，临时存储目录由 spark.local.dir 配置参数指定。 可以通过调整各种配置参数来调整 shuffle 行为。请参阅Spark Configuration Guide中的“shuffle behavior”部分。 RDD 持久化Spark 的一个重要功能就是将数据集持久化（或缓存）到内存中以便在多个操作中重复使用。当我们持久化一个RDD 时，每一个节点将这个 RDD 的每一个分片计算并保存到内存中以便在下次对这个数据集（或者这个数据集衍生的数据集）的计算中可以复用。这使得接下来的计算过程速度能够加快（经常能加快超过十倍的速度）。缓存是加快迭代算法和快速交互过程速度的关键工具。 你可以通过调用 persist() 或 cache() 方法来标记一个想要持久化的 RDD。在第一次被计算产生之后，它就会始终停留在节点的内存中。Spark 的缓存是具有容错性的——如果 RDD 的任意一个分片丢失了，Spark 就会依照这个RDD 产生的转化过程自动重算一遍。 另外，每一个持久化的 RDD 都有一个可变的存储级别，这个级别使得用户可以改变 RDD 持久化的储存位置。比如，你可以将数据集持久化到硬盘上，也可以将它以序列化的 Java 对象形式（节省空间）持久化到内存中，还可以将这个数据集在节点之间复制。这些存储级别都是通过向 persist() 传递一个 StorageLevel 对象（Scala, Java, Python）来设置的。cache() 方法是使用默认存储等级 StorageLevel.MEMORY_ONLY（在内存中存储反序列化对象）的缩写。存储级别的所有种类请见下表： Storage Level Meaning MEMORY_ONLY Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they’re needed. This is the default level. MEMORY_AND_DISK Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don’t fit on disk, and read them from there when they’re needed. MEMORY_ONLY_SER (Java and Scala) Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read. MEMORY_AND_DISK_SER (Java and Scala) Similar to MEMORY_ONLY_SER, but spill partitions that don’t fit in memory to disk instead of recomputing them on the fly each time they’re needed. DISK_ONLY Store the RDD partitions only on disk. MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. Same as the levels above, but replicate each partition on two cluster nodes. OFF_HEAP (experimental) Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled. 注意：在 Python 中，储存的对象永远是通过 Pickle 库序列化过的，所以设不设置序列化级别不会产生影响。在 Python 中可用的存储等级有 MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2, DISK_ONLY, 和 DISK_ONLY_2。 Spark 还会在 shuffle 操作（比如 reduceByKey）中自动储存中间数据，即使用户没有调用 persist。这是为了防止在 shuffle 过程中某个节点出错而导致的全部重算。不过如果用户打算复用某些结果 RDD，我们仍然建议用户对结果RDD手动调用 persist，而不是依赖自动持久化机制。 如何选择存储级别Spark 的存储级别是为了提供内存使用与 CPU 效率之间的不同取舍平衡程度。我们建议用户通过考虑以下流程来选择合适的存储级别： 如果你的 RDD 很适合默认的级别（MEMORY_ONLY），那么久使用默认级别吧。这是 CPU 最高效运行的选择，能够让 RDD 上的操作以最快速度运行。 否则，试试MEMORY_ONLY_SER选项并且 选择一个快的序列化库 来使对象的空间利用率更高，同时尽量保证访问速度足够快。 不要往硬盘上持久化，除非重算数据集的过程代价确实很昂贵，或者这个过程过滤了巨量的数据。否则，重新计算分片有可能跟读硬盘速度一样快。 如果你希望快速的错误恢复（比如用 Spark 来处理 web 应用的请求），使用复制级别。所有的存储级别都提供了重算丢失数据的完整容错机制，但是复制一份副本能省去等待重算的时间。 删除数据Spark 会自动监视每个节点的缓存使用同时使用 LRU 算法丢弃旧数据分片。如果你想手动删除某个 RDD 而不是等待它被自动删除，调用 RDD.unpersist() 方法。 共享变量通常情况下，当一个函数传递给一个在远程集群节点上运行的 Spark 操作（比如 map 和 reduce）时，Spark 会对涉及到的变量的所有副本执行这个函数。这些变量会被复制到每个机器上，而且这个过程不会被反馈给驱动程序。通常情况下，在任务之间读写共享变量是很低效的。但是，Spark 仍然提供了有限的两种共享变量类型用于常见的使用场景：广播变量和累加器。 广播变量广播变量允许程序员在每台机器上保持一个只读变量的缓存而不是将一个变量的拷贝传递给各个任务。它们可以被使用，比如，给每一个节点传递一份大输入数据集的拷贝是很低效的。Spark 试图使用高效的广播算法来分布广播变量，以此来降低通信花销。 Spark Actions 是通过一组阶段执行的，这些阶段由分布式 “shuffle” 操作分开。 Spark 自动广播每个阶段中任务所需的共用数据。 在运行每个任务之前，以这种方式广播的数据以序列化形式缓存并反序列化。 这意味着仅当跨多个阶段的任务需要相同数据或以反序列化形式缓存数据非常重要时，显式创建广播变量才有用。 可以通过SparkContext.broadcast(v)来从变量 v 创建一个广播变量。这个广播变量是 v 的一个包装，同时它的值可以功过调用value方法来获得。以下的代码展示了这一点： 12345&gt;&gt;&gt; broadcastVar = sc.broadcast([1, 2, 3])&lt;pyspark.broadcast.Broadcast object at 0x102789f10&gt;&gt;&gt;&gt; broadcastVar.value[1, 2, 3] 在广播变量被创建之后，在所有函数中都应当使用它来代替原来的变量 v，这样就可以保证 v 在节点之间只被传递一次。另外，v 变量在被广播之后不应该再被修改了，这样可以确保每一个节点上储存的广播变量的一致性（如果这个变量后来又被传输给一个新的节点）。 累加器累加器是在一个相关过程中只能被”累加”的变量，对这个变量的操作可以有效地被并行化。它们可以被用于实现计数器（就像在 MapReduce 过程中）或求和运算。Spark 原生支持对数字类型的累加器，程序员也可以为其他新的类型添加支持。累加器被以一个名字创建之后，会在 Spark 的 UI 中显示出来。这有助于了解计算的累进过程（注意：目前 Python 中不支持这个特性）。 可以通过SparkContext.accumulator(v)来从变量 v 创建一个累加器。在集群中运行的任务随后可以使用 add 方法或 += 操作符（在 Scala 和 Python 中）来向这个累加器中累加值。但是，他们不能读取累加器中的值。只有驱动程序可以读取累加器中的值，通过累加器的 value 方法。 以下的代码展示了向一个累加器中累加数组元素的过程： 12345678910&gt;&gt;&gt; accum = sc.accumulator(0)&gt;&gt;&gt; accumAccumulator&lt;id=0, value=0&gt;&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))...10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s&gt;&gt;&gt; accum.value10 这段代码利用了累加器对 int 类型的内建支持，程序员可以通过继承 AccumulatorParam 类来创建自己想要的类型支持。AccumulatorParam 的接口提供了两个方法：zero 用于为你的数据类型提供零值；addInPlace用于计算两个值得和。比如，假设我们有一个Vector类表示数学中的向量，我们可以这样写： 12345678910class VectorAccumulatorParam(AccumulatorParam): def zero(self, initialValue): return Vector.zeros(initialValue.size) def addInPlace(self, v1, v2): v1 += v2 return v1# Then, create an Accumulator of this type:vecAccum = sc.accumulator(Vector(...), VectorAccumulatorParam()) 累加器的更新操作只会被运行一次，Spark 提供了保证，每个任务中对累加器的更新操作都只会被运行一次。比如，重启一个任务不会再次更新累加器。在转化过程中，用户应该留意每个任务的更新操作在任务或作业重新运算时是否被执行了超过一次。 累加器不会改变 Spark 的惰性求值模型。如果累加器在对 RDD 的操作中被更新了，它们的值只会在 action 操作中作为 RDD 计算过程中的一部分被更新。所以，在一个懒惰的转化操作中调用累加器的更新，并没法保证会被及时运行。下面的代码段展示了这一点： 123456accum = sc.accumulator(0)def g(x): accum.add(x) return f(x)data.map(g)# Here, accum is still 0 because no actions have caused the `map` to be computed. 在集群上部署这个应用提交指南描述了一个应用被提交到集群上的过程。简而言之，只要你把你的应用打成了 JAR 包（ Java/Scala 应用）或 .py 文件的集合或 .zip 压缩包( Python 应用)，bin/spark-submit 脚本会将应用提交到任意支持的集群管理器上。 单元测试Spark 对单元测试是友好的，可以与任何流行的单元测试框架相容。你只需要在测试中创建一个 SparkContext ，并如前文所述将 maste r的 URL 设为 local，执行你的程序，最后调用 SparkContext.stop() 来终止运行。请确保你在 finally 块或测试框架的 tearDown 方法中终止了上下文，因为 Spark 不支持两个上下文在一个程序中同时运行。 其他为了给你优化代码提供帮助，配置指南和调优指南提供了关于最佳实践的一些信息。确保你的数据储存在以高效的格式储存在内存中，这很重要。为了给你部署应用提供帮助，集群模式概览描述了许多内容，包括分布式操作和支持的集群管理器。最后，完整的API文档在这里。Scala, Java, Python 和 R。 unsplash-logoCarolyn V","link":"/posts/8509da34/"},{"title":"大数据平台安装","text":"本文主要记录了 hadoop 大数据集群的相关组件安装及记录过程 基本设置 节点 IP master 192.168.0.100 slave1 192.168.0.101 slave2 192.168.0.102 slave3 192.168.0.103 组件 版本 安装位置 logs 位置 java 1.8 /opt/java/jdk1.8.0_221 hadoop 3.1.2 /opt/hadoop/hadoop-3.1.2 /data/hadoop/logs hive 3.1.2 /opt/hive/apache-hive-3.1.2-bin /data/hive/logs zookeeper 3.5.6 /opt/zookeeper/apache-zookeeper-3.5.6-bin /data/zookeeper/logs kafka 2.12 /opt/kafka/kafka_2.12-2.3.0 /data/kafka/logs flume 1.9.0 /opt/flume/apache-flume-1.9.0-bin /data/flum/logs hbase 2.2.1 /opt/hbase/hbase-2.2.1 /data/hbase/logs spark 2.4.4 /opt/spark/spark-2.4.4-bin-hadoop2.7 /data/spark/logs ssh 互信节点配置123456789101112# 主节点的配置, 其他节点上配置类似# 修改主机名, 退出重新登录即可hostnamectl set-hostname master; exit;# 添加各节点映射vi /etc/hosts192.168.0.100 master192.168.0.101 slave1192.168.0.102 slave2192.168.0.103 slave3 脚本在主节点安装两个脚本到 /usr/local/bin 目录下 脚本一: 集群间同步拷贝文件 cluster_copy_all_nodes 1234567891011121314151617#!/bin/bashSELF=`hostname`if [ -z \"$NODE_LIST\" ]; then echo echo Error: NODE_LIST environment variable must be set in .bash_profile exit 1fifor i in $NODE_LIST; do if [ ! $i = $SELF ]; then if [ $1 = \"-r\" ]; then scp -oStrictHostKeyChecking=no -r $2 $i:$3 else scp -oStrictHostKeyChecking=no $1 $i:$2 fi fidonewait 脚本二: 用来集群间同步运行命令 cluster_run_all_nodes 1234567891011121314151617#!/bin/bashif [ -z \"$NODE_LIST\" ]; then echo echo Error: NODE_LIST environment variable must be set in .bash_profile exit 1fiif [[ $1 = '--background' ]]; then shift for i in $NODE_LIST; do ssh -oStrictHostKeyChecking=no -n $i \"$@\" &amp; doneelse for i in $NODE_LIST; do ssh -oStrictHostKeyChecking=no $i \"$@\" donefiwait 授予两个脚本的可执行权限 1chmod +x /usr/local/bin/cluster_* 配置环境变量配置主节点环境变量 1234vi ~/.bash_profileexport NODE_LIST='master slave1 slave2 slave3'source ~/.bash_profile 配置集群ssh互信1. 各节点生成密钥和公钥 1cluster_run_all_nodes \"hostname; ssh-keygen -q -t rsa -N \\\"\\\" -f ~/.ssh/id_rsa\" 如果配置有误，或者清除ssh互信的当前所有配置信息 12cluster_run_all_nodes \"hostname ; rm -rf ~/.ssh\"rm -rf ~/.ssh 2. 将所有的公钥文件汇总到一个总的授权key文件中 在主节点上汇总, 直接 shell 执行 123456IP_NET=\"192.168.0.\" for((i=100;i&lt;=103;i++))dossh $IP_NET$i cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keysecho Summarize ssh info from $IP_NET$i into a single file.done 出于安全性考虑，将这个授权key文件赋予600权限 1chmod 600 ~/.ssh/authorized_keys 3. 将这个包含了所有互信机器认证key的认证文件，分发到各个机器中去 1cluster_copy_all_nodes ~/.ssh/authorized_keys ~/.ssh/ 4. 验证ssh互信 master 上运行，都不输入密码返回主机名和时间即可 1cluster_run_all_nodes \"hostname;date\" 结尾至此，ssh集群间的互信已经配置完成。但为了更加灵活的再其他节点也可以用到我们自定义的脚本，我们还可以做以下工作 同步拷贝 master 的配置文件到其他节点 1234cluster_copy_all_nodes ~/.bash_profile ~/cluster_copy_all_nodes /etc/hosts /etccluster_copy_all_nodes /usr/local/bin/cluster_copy_all_nodes /usr/local/bin/cluster_copy_all_nodes /usr/local/bin/cluster_run_all_nodes /usr/local/bin/ 这时任意登录其他节点，也可以使用cluster_run_all_nodes验证ssh互信了 1cluster_run_all_nodes \"hostname;date\" Hadoop关闭防火墙1234567# 关闭 SELINUXvi /etc/selinux/configSELINUX=disabled# 关闭防火墙systemctl stop firewalldsystemctl disable firewalld Java 安装配置1234567891011121314151617# 之后我们所有的环境配置包都放到/opt/下# 新建java目录，把下载好的jdk的二进制包拷到下面mkdir -p /opt/javatar -zxvf jdk-8u221-linux-x64.tar.gz -C /opt/java/# 配置环境变量，在profile文件最后添加java的环境变量vim /etc/profileexport JAVA_HOME=/opt/java/jdk1.8.0_221export PATH=$PATH:$JAVA_HOME/binsource /etc/profilejava -versionjava version \"1.8.0_221\"Java(TM) SE Runtime Environment (build 1.8.0_221-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.221-b11, mixed mode) 主节点1. 下载解压 123# 在/opt 下创建hadoop文件夹，将下载好的 hadoop 压缩包上传进去解压mkdir -p /opt/hadooptar -zxvf hadoop-3.2.1.tar.gz -C /opt/hadoop 2. 配置环境变量 12345678910111213vi /etc/profileexport HADOOP_HOME=/opt/hadoop/hadoop-3.2.1export HADOOP_INSTALL=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport YARN_HOME=$HADOOP_HOMEexport YARN_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 3. 配置 NAMENODE 12345678910111213141516# 配置文件主要在 HADOOP_CONF_DIR 下面vi $HADOOP_CONF_DIR/core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hadoop&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;# fs.defaultFS指定了hdfs的默认路径；hadoop.tmp.dir指定了hadoop文件存放路径的根目录, 需手动创建 4. 配置HDFS 12345678910111213141516171819vi $HADOOP_CONF_DIR/hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;master:50091&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;可以不用设置dfs.namenode.name.dir和dfs.datanode.data.dir，因为它们的值/data/hadoop会继承自core-site.xml中的hadoop.tmp.dir。dfs.replication设置副本数量，因为3节点中只有2个DataNode，因此此处为2，默认是3。 5. 配置 MapReduce 12345678910vi $HADOOP_CONF_DIR/mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;YARN是一个通用的资源协调器，不仅可以为MapReduce服务，也可以为Spark、Tez等应用服务。 6. 配置 YARN 123456789101112vi $HADOOP_CONF_DIR/yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 7. 配置 workers 123456# 我们使用 master 来作为 NameNode, slave1 slave2 slave3 作为 DataNode。workers默认只有一行内容: localhost，使用 vi 编辑 slaves，并写入下面内容：vi $HADOOP_CONF_DIR/workersslave1slave2slave3 8. 配置 hadoop-env.sh 1234vi $HADOOP_CONF_DIR/hadoop-env.shexport HADOOP_LOG_DIR=/data/hadoop/logsexport JAVA_HOME=/opt/java/jdk1.8.0_221 子节点1. 创建 数据存储目录 1cluster_run_all_nodes \"hostname; mkdir -p /data/hadoop\" 2. 将 java 复制到子节点 1cluster_copy_all_nodes /opt/hadoop /opt 3. 将 hadoop 复制到子节点 1234# hadoop 目录下 share/doc 为文档并且比较大, 先删除, 再复制到其他节点rm -rf /opt/hadoop/hadoop-3.2.1/share/doc/cluster_copy_all_nodes -r /opt/hadoop/ /opt 4. 复制配置文件到子节点 1cluster_copy_all_nodes /etc/profile /etc/ 5. 刷新环境 1cluster_run_all_nodes \"hostname; source /etc/profile\" 6. 修改 shell 脚本（option） 使用 root 用户安装的 hadoop, 需要修改 /sbin/ 中以下文件 1234567891011121314# 对于 start-dfs.sh 和 stop-dfs.sh 文件，添加下列参数#!/usr/bin/env bashHDFS_DATANODE_USER=rootHADOOP_SECURE_DN_USER=hdfsclu HDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=root# 对于start-yarn.sh和stop-yarn.sh文件，添加下列参数#!/usr/bin/env bashYARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=yarnYARN_NODEMANAGER_USER=root 测试是否配置成功12345678910111213141516171819202122# 只要在主节点上启动，执行过程可能稍慢，耐心等待# 格式化 namenode# 这个命令非常危险！你应当只在新建集群后执行一次，因为 namenode 保存的是 HDFS 的所有元信息，如果丢失了，整个集群中 DataNode 的数据都无法访问，就算它们还在磁盘上hdfs namenode -format# 启动 hadoop# hadoop 所有启动文件脚本都在 /sbin 下cd $HADOOP_HOME/sbin./start-all.sh# 主节点输入 jps6517 Jps5832 NameNode5289 ResourceManager6106 SecondaryNameNode# 从节点输入 jps2931 NodeManager3068 Jps2815 DataNode# 至此, hadoop 搭建完成 web uinamenode 在 hadoop3.0 以后默认端口为 9870 不是 50070 namenode ResourceManager Hive环境配置12345678910111213# 注意：Hive只需要在master节点上安装配置mkdir -p /opt/hivetar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/hive# 添加环境变量vi /etc/profileexport HIVE_HOME=/opt/hive/apache-hive-3.1.2-binexport PATH=$PATH:$HIVE_HOME/bin# 刷新环境变量source /etc/profile 修改 hive-site.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465vi $HIVE_HOME/conf/hive-site.xml&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt; JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/opt/hive/apache-hive-3.1.2-bin/tmp/hadoop&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt; &lt;value&gt;/opt/hive/apache-hive-3.1.2-bin/tmp/hadoop/operation_logs&lt;/value&gt; &lt;description&gt;Top level directory where operation logs are stored if logging functionality is enabled&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/opt/hive/apache-hive-3.1.2-bin/tmp/hadoop&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/opt/hive/apache-hive-3.1.2-bin/tmp/${hive.session.id}_resources&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt; Enforce metastore schema version consistency. True: Verify that version information stored in is compatible with one from Hive jars. Also disable automatic schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures proper metastore schema migration. (Default) False: Warn if the version information stored in metastore doesn't match with one from in Hive jars. &lt;/description&gt; &lt;/property&gt; 修改 hive-env.sh 文件12345678910111213cp $HIVE_HOME/conf/hive-env.sh.template $HIVE_HOME/conf/hive-env.shvi $HIVE_HOME/conf/hive-env.sh# Set HADOOP_HOME to point to a specific hadoop install directoryHADOOP_HOME=/opt/hadoop/hadoop-3.2.1# Hive Configuration Directory can be controlled by:export HIVE_CONF_DIR=/opt/hive/apache-hive-3.1.2-bin/conf# Folder containing extra libraries required for hive compilation/execution can be controlled by:# export HIVE_AUX_JARS_PATH=export JAVA_HOME=/opt/java/jdk1.8.0_221/export HIVE_HOME=/opt/hive/apache-hive-3.1.2-bin/ 设置 logs123456cp $HIVE_HOME/conf/hive-log4j2.properties.template $HIVE_HOME/conf/hive-log4j2.propertiesvi $HIVE_HOME/conf/hive-log4j2.propertiesproperty.hive.log.dir = /data/hive/logsmkdir -p /data/hive/logs 安装并配置 mysql12345678910111213141516171819202122232425262728293031323334353637wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpmrpm -ivh mysql-community-release-el7-5.noarch.rpmyum search mysql# 里面有一条# mysql-community-server.x86_64 : The MySQL server and related files# 安装它mysql-community-server.x86_64# 刚才yum search的时候,有一条# mysql-connector-java.noarch : Official JDBC driver for MySQL# 安装它yum install mysql-connector-java.noarch# 安装好后在 /usr/share/java 下# 拷贝到 hive 的 lib 文件夹下cp /usr/share/java/mysql-connector-java.jar $HIVE_HOME/lib# 重启 mysql 服务service mysqld restartmysqlWelcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 2Server version: 5.6.46 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt;# 安装成功 在mysql上创建hive元数据库，创建hive账号，并进行授权 1234567# 在mysql上连续执行下述命令# use mysql;# update user set password=password('123456') where user='root';# flush privileges;# 重新连接 mysql -u root -p# create database if not exists hive_metadata; 初始化1schematool -dbType mysql -initSchema 测试123# 输入 hive# 进入 hive 交互界面# 安装成功 Zookeeper环境配置12345678910# 新建文件夹mkdir -p /opt/zookeeper# 解压tar -zxvf apache-zookeeper-3.5.6-bin.tar.gz -C /opt/zookeeper# 配置环境变量vi /etc/profileexport ZOOKEEPER_HOME=/opt/zookeeper/apache-zookeeper-3.5.6-binexport PATH=$PATH:$ZOOKEEPER_HOME/bin 配置 zoo.cfg 文件1234567891011cp $ZOOKEEPER_HOME/conf/zoo.sample.cfg $ZOOKEEPER_HOME/conf/zoo.cfgvi $ZOOKEEPER_HOME/conf/zoo.cfgdataDir=/data/zookeeperserver.0=master:2888:3888server.1=slave1:2888:3888 server.2=slave2:2888:3888server.3=slave3:2888:3888# server. # 后面不能有空格！！！！！ 配置 myid 文件1234567# 创建 dataDir 目录mkdir -p /data/zookeeper# 创建 myid 文件# 修改其中的内容vi /data/zookeeper/myid0 设置 logs 目录12345vi log4j.propertieszookeeper.log.dir=/data/zookeeper/logsmkdir -p /data/zookeeper/logs 配置其他节点12345678910# 把 mster 上配置好的 zookeeper 文件夹直接传到其他节点cluster_copy_all_nodes -r /opt/zookeeper/ /opt# 把 /data/zookeeper 传到其他节点cluster_copy_all_nodes -r /data/zookeeper/ /data# 修改子节点上 myid 的内容# 配置子节点上 /etc/profile# 刷新环境变量 测试1234567891011# 在所有节点上分别执行命令# 启动服务zkServer.sh start# 查看状态zkServer.sh status# 正确结果应该是: 三个节点中其中一个是leader, 另外两个是follower# 检查三个节点是否都有QuromPeerMain进程jps Kafka安装 scala1234567891011121314151617181920# 解压mkdir -p /opt/scalatar -zxvf scala-2.12.10.tgz -C /opt/scala/# 配置环境变量vi /etc/profileexport SCALA_HOME=/opt/scala/scala-2.12.10export PATH=$PATH:$SCALA_HOME/binsource /etc/profile# 验证scala -versionScala code runner version 2.12.10 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc.# 将 scala 安装到其他节点cluster_copy_all_nodes -r /opt/scala/ /optcluster_copy_all_nodes /etc/profile /etccluster_run_all_nodes \"hostname; source /etc/profile\" 安装配置 kafka123456789101112131415161718192021222324252627282930313233343536# 创建目录# 解压文件mkdir -p /opt/kafkatar -zxvf kafka_2.12-2.3.0.tgz -C /opt/kafka/# 配置环境变量vi /etc/profileexport KAFKA_HOME=/opt/kafka/kafka_2.12-2.3.0export PATH=$PATH:$KAFKA_HOME/bin# 修改 server.properties 文件, 找到对应的位置, 修改如下vi $KAFKA_HOME/config/server.propertiesbroker.id=0listeners=PLAINTEXT://192.168.0.100:9092advertised.listeners=PLAINTEXT://192.168.0.100:9092log.dirs=/data/kafka/logszookeeper.connect=192.168.0.100:2181,192.168.0.101:2181,192.168.0.102:2181,192.168.0.103:2181# 修改 zookeeper.properties 文件, 找到对应的位置, 修改如下dataDir=/data/kafka/zookeeper# 创建相关文件夹mkdir -p /data/kafka/zookeepermkdir -p /data/kafka/logs# 同步文件到其他节点# 在另外节点上，对server.properties要有几处修改# broker.id 分别修改成： 1 2 3# listeners 在 ip 那里分别修改成子节点对应的 ip# advertised.listeners 也在 ip 那里分别修改成子节点对应的 ip# zookeeper.connect 不需要修改# 另外节点上也别忘了配置kafka环境变量cluster_copy_all_nodes -r /opt/kafka/ /optcluster_copy_all_nodes -r /data/kafka/ /datacluster_copy_all_nodes /etc/profile /etc 测试123456789101112131415161718# 在所有节点上都启动 kafkanohup kafka-server-start.sh $KAFKA_HOME/config/server.properties &amp;# 在主节点上创建主题TestTopickafka-topics.sh --zookeeper 192.168.0.100:2181,192.168.0.101:2181,192.168.0.102:2181,192.168.0.103:2181 --topic TestTopic --replication-factor 1 --partitions 1 --create# 在主节点上启动一个生产者kafka-console-producer.sh --broker-list 192.168.0.100:9092,192.168.0.101:9092,192.168.0.102:9092,192.168.0.103:9092 --topic TestTopic# 在其他节点上分别创建消费者kafka-console-consumer.sh --bootstrap-server 192.168.0.101:9092 --topic TestTopic --from-beginningkafka-console-consumer.sh --bootstrap-server 192.168.0.102:9092 --topic TestTopic --from-beginningkafka-console-consumer.sh --bootstrap-server 192.168.0.103:9092 --topic TestTopic --from-beginning# 在主节点生产者命令行那里随便输入一段话&gt; hello world# 然后你就会发现在其他两个消费者节点那里也出现了这句话, 即消费到了该数据 Flume环境配置1234567891011# flume只需要在主节点配置, 不需要在其他节点配置# 创建目录, 解压文件mkdir -p /opt/flumetar -zxvf apache-flume-1.9.0-bin.tar.gz -C /opt/flume/# 环境变量vi /etc/profileexport FLUME_HOME=/opt/flume/apache-flume-1.9.0-binexport FLUME_CONF_DIR=$FLUME_HOME/confexport PATH=$PATH:$FLUME_HOME/bin 修改 flume-conf.properties12345678910111213141516171819202122232425262728293031323334cp $FLUME_CONF_DIR/flume-conf.properties.template $FLUME_CONF_DIR/flume-conf.properties# 在文件追加vi $FLUME_CONF_DIR/flume-conf.properties#agent1表示代理名称agent1.sources=source1agent1.sinks=sink1agent1.channels=channel1#配置source1agent1.sources.source1.type=spooldiragent1.sources.source1.spoolDir=/data/flume/logsagent1.sources.source1.channels=channel1agent1.sources.source1.fileHeader = falseagent1.sources.source1.interceptors = i1agent1.sources.source1.interceptors.i1.type = timestamp#配置channel1agent1.channels.channel1.type=fileagent1.channels.channel1.checkpointDir=/data/flume/logs_tmp_cpagent1.channels.channel1.dataDirs=/data/flume/logs_tmp#配置sink1agent1.sinks.sink1.type=hdfsagent1.sinks.sink1.hdfs.path=hdfs://master:9000/logsagent1.sinks.sink1.hdfs.fileType=DataStreamagent1.sinks.sink1.hdfs.writeFormat=TEXTagent1.sinks.sink1.hdfs.rollInterval=1agent1.sinks.sink1.channel=channel1agent1.sinks.sink1.hdfs.filePrefix=%Y-%m-%d# 我们看到上面的配置文件中代理 agent1.sources.source1.spoolDir 监听的文件夹是 /data/flume/logs, 所以我们要手动创建一下mkdir -p /data/flume/logs# 上面的配置文件中 agent1.sinks.sink1.hdfs.path=hdfs://master:9000/logs 下，即将监听到的 /data/flume/logs 下的文件自动上传到 hdfs 的 /logs 下, 所以我们要手动创建hdfs 下的目录hdfs dfs -mkdir /logs 测试12345678910111213141516171819202122# 删除rm $FLMUE_HOME/lib/slf4j-log4j12-1.7.25.jar# 启动服务nohup flume-ng agent -n agent1 -c conf -f $FLUME_CONF_DIR/flume-conf.properties -Dflume.root.logger=DEBUG,console &amp;# 先看下hdfs的logs目录下, 目前什么都没有drwxr-xr-x - root supergroup 0 2019-10-20 16:53 /logsdrwxr-xr-x - root supergroup 0 2019-10-20 16:53 /logs# 我们在 /data/flume/logs 随便创建个文件vi /data/flume/logs/test.txthunan universitygallon# 然后我们发现hdfs的logs下自动上传了我们刚刚创建的文件hdfs dfs -ls -R /-rw-r--r-- 2 root supergroup 24 2019-10-20 16:45 /logs/2019-10-20.1571561139240hdfs dfs -cat /logs/2019-10-20.1571561139240hunan universitygallon Hbase环境配置123456789101112# 创建目录# 解压文件mkdir -p /opt/hbasetar -zxvf hbase-2.2.1-bin.tar.gz -C /opt/hbase# 环境变量vi /etc/profileexport HBASE_HOME=/opt/hbase/hbase-2.2.1export PATH=$PAHT:$HBASE_HOME/binsource /etc/profile 修改 hbase-env.sh 文件12345vi $HBASE_HOME/conf/hbase-env.sh export JAVA_HOME=/opt/java/jdk1.8.0_221export HBASE_LOG_DIR=/data/hbase/logs export HBASE_MANAGES_ZK=false 修改 hbase-site.xml 文件12345678910111213141516171819202122232425262728293031323334353637vi $HBASE_HOME/conf/hbase-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master,slave1,slave2,slave3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hbase/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master&lt;/name&gt; &lt;value&gt;hdfs://master:60000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;16010&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.regionserver.info.port&lt;/name&gt; &lt;value&gt;16030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;完全分布式式必须为false&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 regionservers 文件123456vi $HBASE_HOME/conf/regionservers masterslave1slave2slave3 其他节点配置123456789# 复制文件cluster_copy_all_nodes -r /opt/hbase/ /optcluster_copy_all_nodes -r /data/hbase/ /datacluster_copy_all_nodes /etc/profile /etc# 刷新环境cluster_run_all_nodes \"hostname; source /etc/profile\"cluster_run_all_nodes \"rm /opt/hbase/hbase-2.2.1/lib/client-facing-thirdparty/slf4j-log4j12-1.7.25.jar\" 测试123456rm /opt/hbase/hbase-2.2.1/lib/client-facing-thirdparty/slf4j-log4j12-1.7.25.jar# 注意：测试Hbase之前, zookeeper 和 hadoop 需要提前启动起来start-hbase.sh # 正确结果：主节点上显示：HMaster / 子节点上显示：HRegionServer 在浏览器中访问 web ui Spark环境配置123456789# 创建目录, 解压文件mkdir -p /opt/sparktar -zxvf spark-2.4.4-bin-hadoop2.7.tgz -C /opt/spark/# 配置环境变量vi /etc/profileexport SPARK_HOME=/opt/spark/spark-2.4.4-bin-hadoop2.7export PATH=$PATH:$SPARK_HOME/sbin:$SPARK_HOME/bin 修改 spark-env.sh 文件1234567891011121314cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.shvi $SPARK_HOME/conf/spark-env.shSPARK_LOG_DIR=/data/spark/logsexport JAVA_HOME=/opt/java/jdk1.8.0_221export SCALA_HOME=/opt/scala/scala-2.12.10export HADOOP_HOME=/opt/hadoop/hadoop-3.1.2export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopmkdir -p /data/spark/logs# 文件重命名mv $SPARK_HOME/sbin/start-all.sh $SPARK_HOME/sbin/start-spark-all.shmv $SPARK_HOME/sbin/stop-all.sh $SPARK_HOME/sbin/stop-spark-all.sh 修改 slaves 文件1234567cp $SPARK_HOME/conf/slaves.template $SPARK_HOME/conf/slavesvi $SPARK_HOME/conf/slavesmasterslave1slave2slave3 其他节点配置12cluster_copy_all_nodes -r /opt/spark/ /opt/cluster_copy_all_nodes /etc/profile /etc 启动12345vi start-master.shSPARK_MASTER_WEBUI_PORT=8001./start-all.sh 启动完毕后在主机浏览器访问界面 unsplash-logoJOHN TOWNER","link":"/posts/f9bc2212/"},{"title":"shell相关","text":"我使用shell过程中碰到的一些问题，记录在此以便查询。 shell中各种括号的作用单小括号 () 命令组。括号中的命令将会新开一个子shell顺序执行，所以括号中的变量不能够被脚本余下的部分使用。括号中多个命令之间用分号隔开，最后一个命令可以没有分号，各命令和括号之间不必有空格。 命令替换。等同于cmd，shell扫描一遍命令行，发现了$(cmd)结构，便将$(cmd)中的cmd执行一次，得到其标准输出，再将此输出放到原来命令。有些shell不支持，如tcsh。 用于初始化数组。如：array=(a b c d) 双小括号 (()) 整数扩展。这种扩展计算是整数型的计算，不支持浮点型。((exp))结构扩展并计算一个算术表达式的值，如果表达式的结果为0，那么返回的退出状态码为1，或者是”假”，而一个非零值的表达式所返回的退出状态码将为0，或者是”true”。若是逻辑判断，表达式exp为真则为1,假则为0。 只要括号中的运算符、表达式符合C语言运算规则，都可用在$((exp))中，甚至是三目运算符。作不同进位(如二进制、八进制、十六进制)运算时，输出结果全都自动转化成了十进制。如：echo $((16#5f)) 结果为95 (16进位转十进制) 单纯用 (( )) 也可重定义变量值，比如 a=5; ((a++)) 可将 $a 重定义为6 常用于算术运算比较，双括号中的变量可以不使用$符号前缀。括号内支持多个表达式用逗号分开。 只要括号中的表达式符合C语言运算规则,比如可以直接使用for((i=0;i&lt;5;i++)), 如果不使用双括号, 则为for i in seq 0 4或者for i in {0..4}。再如可以直接使用if ((​$i&lt;5)), 如果不使用双括号, 则为if [ $i -lt 5 ]。 单中括号 [] bash 的内部命令，[和test是等同的。如果我们不用绝对路径指明，通常我们用的都是bash自带的命令。if/test结构中的左中括号是调用test的命令标识，右中括号是关闭条件判断的。这个命令把它的参数作为比较表达式或者作为文件测试，并且根据比较的结果来返回一个退出状态码。if/test结构中并不是必须右中括号，但是新版的Bash中要求必须这样。 Test和[]中可用的比较运算符只有==和!=，两者都是用于字符串比较的，不可用于整数比较，整数比较只能使用-eq，-gt这种形式。无论是字符串比较还是整数比较都不支持大于号小于号。如果实在想用，对于字符串比较可以使用转义形式，如果比较”ab”和”bc”：[ ab &lt; bc ]，结果为真，也就是返回状态为0。[ ]中的逻辑与和逻辑或使用-a 和-o 表示。 字符范围。用作正则表达式的一部分，描述一个匹配的字符范围。作为test用途的中括号内不能使用正则。 在一个array 结构的上下文中，中括号用来引用数组中每个元素的编号。 双中括号 [[]] [[是 bash 程序语言的关键字。并不是一个命令，[[ ]] 结构比[ ]结构更加通用。在[[和]]之间所有的字符都不会发生文件名扩展或者单词分割，但是会发生参数扩展和命令替换。 支持字符串的模式匹配，使用=~操作符时甚至支持shell的正则表达式。字符串比较时可以把右边的作为一个模式，而不仅仅是一个字符串，比如[[ hello == hell? ]]，结果为真。[[ ]] 中匹配字符串或通配符，不需要引号。 使用[[ … ]]条件判断结构，而不是[ … ]，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt;和&gt; 操作符能够正常存在于[[ ]]条件判断结构中，但是如果出现在[ ]结构中的话，会报错。比如可以直接使用if [[ $a != 1 &amp;&amp; $a != 2 ]], 如果不适用双括号, 则为if [ ​$a -ne 1] &amp;&amp; [ $a != 2 ]或者if [ $a -ne 1 -a $a -ne 2 ]。 bash把双中括号中的表达式看作一个单独的元素，并返回一个退出状态码。 例子 123456789if ($i&lt;5) if [ $i -lt 5 ] if [ $a -ne 1 -a $a != 2 ] if [ $a -ne 1] &amp;&amp; [ $a != 2 ] if [[ $a != 1 &amp;&amp; $a != 2 ]] for i in $(seq 0 4);do echo $i;donefor i in `seq 0 4`;do echo $i;donefor ((i=0;i&lt;5;i++));do echo $i;donefor i in {0..4};do echo $i;done 大括号 {}常规用法 大括号拓展。(通配(globbing))将对大括号中的文件名做扩展。在大括号中，不允许有空白，除非这个空白被引用或转义。第一种：对大括号中的以逗号分割的文件列表进行拓展。如 touch {a,b}.txt 结果为a.txt b.txt。第二种：对大括号中以点点（..）分割的顺序文件列表起拓展作用，如：touch {a..d}.txt 结果为a.txt b.txt c.txt d.txt 123456# ls {ex1,ex2}.sh ex1.sh ex2.sh # ls {ex{1..3},ex4}.sh ex1.sh ex2.sh ex3.sh ex4.sh # ls {ex[1-3],ex4}.sh ex1.sh ex2.sh ex3.sh ex4.sh 代码块，又被称为内部组，这个结构事实上创建了一个匿名函数 。与小括号中的命令不同，大括号内的命令不会新开一个子shell运行，即脚本余下部分仍可使用括号内变量。括号内的命令间用分号隔开，最后一个也必须有分号。{}的第一个命令和左括号之间必须要有一个空格。 1${var:-string},${var:+string},${var:=string},${var:?string} ${var:-string}和${var:=string}:若变量var为空，则用在命令行中用string来替换${var:-string}，否则变量var不为空时，则用变量var的值来替换${var:-string}；对于${var:=string}的替换规则和${var:-string}是一样的，所不同之处是${var:=string}若var为空时，用string替换${var:=string}的同时，把string赋给变量var： ${var:=string}很常用的一种用法是，判断某个变量是否赋值，没有的话则给它赋上一个默认值。 ${var:+string}的替换规则和上面的相反，即只有当var不是空的时候才替换成string，若var为空时则不替换或者说是替换成变量 var的值，即空值。(因为变量var此时为空，所以这两种说法是等价的) ${var:?string}替换规则为：若变量var不为空，则用变量var的值来替换${var:?string}；若变量var为空，则把string输出到标准错误中，并从脚本中退出。我们可利用此特性来检查是否设置了变量的值。 补充扩展：在上面这几种替换结构中string不一定是常值的，可用另外一个变量的值或是一种命令的输出。 模式匹配替换结构模式匹配记忆方法： # 是去掉左边(在键盘上#在$之左边)% 是去掉右边(在键盘上%在$之右边)# 和 %中的单一符号是最小匹配，两个相同符号是最大匹配。 1${var%pattern},${var%%pattern},${var#pattern},${var##pattern} 第一种模式：${variable%pattern}，这种模式时，shell在variable中查找，看它是否是给的模式pattern结尾，如果是，就从命令行把variable中的内容去掉右边最短的匹配模式 第二种模式：${variable%%pattern}，这种模式时，shell在variable中查找，看它是否是给的模式pattern结尾，如果是，就从命令行把variable中的内容去掉右边最长的匹配模式 第三种模式：${variable#pattern} 这种模式时，shell在variable中查找，看它是否是给的模式pattern开始，如果是，就从命令行把variable中的内容去掉左边最短的匹配模式 第四种模式： ${variable##pattern}这种模式时，shell在variable中查找，看它是否是给的模式pattern结尾，如果是，就从命令行把variable中的内容去掉右边最长的匹配模式 这四种模式中都不会改变variable的值，其中，只有在pattern中使用了*匹配符号时，%和%%，#和##才有区别。结构中的pattern支持通配符，*表示零个或多个任意字符，?表示仅与一个任意字符匹配，[…]表示匹配中括号里面的字符，[!…]表示不匹配中括号里面的字符。 123456789101112131415161718# var=testcase # echo $var testcase # echo ${var%s*e} testca # echo $var testcase # echo ${var%%s*e} te # echo ${var#?e} stcase # echo ${var##?e} stcase # echo ${var##*e}# echo ${var##*s} e # echo ${var##test} case 字符串提取和替换1${var:num},${var:num1:num2},${var/pattern/pattern},${var//pattern/pattern} 第一种模式：${var:num}，这种模式时，shell在var中提取第num个字符到末尾的所有字符。若num为正数，从左边0处开始；若num为负数，从右边开始提取字串，但必须使用在冒号后面加空格或一个数字或整个num加上括号，如${var: -2}、​${var:1-3}或​${var:(-2)}。 第二种模式：${var:num1:num2}，num1是位置，num2是长度。表示从$var字符串的第​$num1个位置开始提取长度为$num2的子串。不能为负数。 第三种模式：${var/pattern/pattern}表示将var字符串的第一个匹配的pattern替换为另一个pattern。 第四种模式：${var//pattern/pattern}表示将var字符串中的所有能匹配的pattern替换为另一个pattern。 123456789101112131415# var=/home/centos # echo $var /home/centos# echo ${var:5} /centos# echo ${var: -6} centos # echo ${var:(-6)} centos # echo ${var:1:4} home # echo ${var/o/h} /hhme/centos# echo ${var//o/h} /hhme/cenths $后的括号 ${a} 变量a的值, 在不引起歧义的情况下可以省略大括号。 $(cmd) 命令替换，和cmd效果相同，结果为shell命令cmd的输，过某些Shell版本不支持$()形式的命令替换, 如tcsh。 $((expression)) 和exprexpression效果相同, 计算数学表达式exp的数值, 其中exp只要符合C语言的运算规则即可, 甚至三目运算符和逻辑表达式都可以计算。 多条命令执行 单小括号，(cmd1;cmd2;cmd3) 新开一个子shell顺序执行命令cmd1,cmd2,cmd3, 各命令之间用分号隔开, 最后一个命令后可以没有分号。 单大括号，{ cmd1;cmd2;cmd3;} 在当前shell顺序执行命令cmd1,cmd2,cmd3, 各命令之间用分号隔开, 最后一个命令后必须有分号, 第一条命令和左括号之间必须用空格隔开。 对{}和()而言, 括号中的重定向符只影响该条命令， 而括号外的重定向符影响到括号中的所有命令。 shell编程四剑客findLinux find命令用来在指定目录下查找文件。任何位于参数之前的字符串都将被视为欲查找的目录名。如果使用该命令时，不设置任何参数，则find命令将在当前目录下查找子目录与文件。并且将查找到的子目录和文件全部进行显示。 语法 find path -option [ -print ] [ -exec -ok command ] {} ; 参数说明 : find 根据下列规则判断 path 和 expression，在命令列上第一个 - ( ) , ! 之前的部份为 path，之后的是 expression。如果 path 是空字串则使用目前路径，如果 expression 是空字串则使用 -print 为预设 expression。 expression 中可使用的选项有二三十个之多，在此只介绍最常用的部份。 -mount, -xdev : 只检查和指定目录在同一个文件系统下的文件，避免列出其它文件系统中的文件 -amin n : 在过去 n 分钟内被读取过 -anewer file : 比文件 file 更晚被读取过的文件 -atime n : 在过去n天内被读取过的文件 -cmin n : 在过去 n 分钟内被修改过 -cnewer file :比文件 file 更新的文件 -ctime n : 在过去n天内被修改过的文件 -empty : 空的文件-gid n or -group name : gid 是 n 或是 group 名称是 name -ipath p, -path p : 路径名称符合 p 的文件，ipath 会忽略大小写 -name name, -iname name : 文件名称符合 name 的文件。iname 会忽略大小写 -size n : 文件大小 是 n 单位，b 代表 512 位元组的区块，c 表示字元数，k 表示 kilo bytes，w 是二个位元组。-type c : 文件类型是 c 的文件。 d: 目录 c: 字型装置文件 b: 区块装置文件 p: 具名贮列 f: 一般文件 l: 符号连结 s: socket -pid n : process id 是 n 的文件 你可以使用 ( ) 将运算式分隔，并使用下列运算。 exp1 -and exp2 ! expr -not expr exp1 -or exp2 exp1, exp2 例子12345678# 查找home目录下名字为test的所有txt文件find /home/ -name \"test.txt\"# 查找当前路径下，一天内创建的，名字以.txt结尾的目录（f表示文件），并将它复制到tmp目录下find . -name \"*.txt\" -type d -mtime -1 -exec cp -r {} /tmp/ \\;# 查找当前路径下30天以前的后缀是log的文件，然后删除掉find . -type f -name \"*.log\" -mtime +30 -exec rm -rf {} \\; grepLinux grep 命令用于查找文件里符合条件的字符串。 grep 指令用于查找内容包含指定的范本样式的文件，如果发现某文件的内容符合所指定的范本样式，预设 grep 指令会把含有范本样式的那一列显示出来。若不指定任何文件名称，或是所给予的文件名为 -，则 grep 指令会从标准输入设备读取数据。 语法 grep [-abcEFGhHilLnqrsvVwxy][-A&lt;显示列数&gt;][-B&lt;显示列数&gt;][-C&lt;显示列数&gt;][-d&lt;进行动作&gt;][-e&lt;范本样式&gt;][-f&lt;范本文件&gt;][–help][范本样式][文件或目录…] 参数： -a 或 –text : 不要忽略二进制的数据。 -A&lt;显示行数&gt; 或 –after-context=&lt;显示行数&gt; : 除了显示符合范本样式的那一列之外，并显示该行之后的内容。 -b 或 –byte-offset : 在显示符合样式的那一行之前，标示出该行第一个字符的编号。 -B&lt;显示行数&gt; 或 –before-context=&lt;显示行数&gt; : 除了显示符合样式的那一行之外，并显示该行之前的内容。 -c 或 –count : 计算符合样式的列数。 -C&lt;显示行数&gt; 或 –context=&lt;显示行数&gt;或-&lt;显示行数&gt; : 除了显示符合样式的那一行之外，并显示该行之前后的内容。 -d &lt;动作&gt; 或 –directories=&lt;动作&gt; : 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep指令将回报信息并停止动作。 -e&lt;范本样式&gt; 或 –regexp=&lt;范本样式&gt; : 指定字符串做为查找文件内容的样式。 -E 或 –extended-regexp : 将样式为延伸的正则表达式来使用。 -f&lt;规则文件&gt; 或 –file=&lt;规则文件&gt; : 指定规则文件，其内容含有一个或多个规则样式，让grep查找符合规则条件的文件内容，格式为每行一个规则样式。 -F 或 –fixed-regexp : 将样式视为固定字符串的列表。 -G 或 –basic-regexp : 将样式视为普通的表示法来使用。 -h 或 –no-filename : 在显示符合样式的那一行之前，不标示该行所属的文件名称。 -H 或 –with-filename : 在显示符合样式的那一行之前，表示该行所属的文件名称。 -i 或 –ignore-case : 忽略字符大小写的差别。 -l 或 –file-with-matches : 列出文件内容符合指定的样式的文件名称。 -L 或 –files-without-match : 列出文件内容不符合指定的样式的文件名称。 -n 或 –line-number : 在显示符合样式的那一行之前，标示出该行的列数编号。 -o 或 –only-matching : 只显示匹配PATTERN 部分。 -q 或 –quiet或–silent : 不显示任何信息。 -r 或 –recursive : 此参数的效果和指定”-d recurse”参数相同。 -s 或 –no-messages : 不显示错误信息。 -v 或 –revert-match : 显示不包含匹配文本的所有行。 -V 或 –version : 显示版本信息。 -w 或 –word-regexp : 只显示全字符合的列。 -x –line-regexp : 只显示全列符合的列。 -y : 此参数的效果和指定”-i”参数相同。 例子12345# 查找passwd中的root字符并加行号加颜色显示grep -n --color \"root\" /etc/passwd# 查找文件中不含#的行grep -v \"#\" /usr/data.txt awkAWK是一种处理文本文件的语言，是一个强大的文本分析工具。 之所以叫AWK是因为其取了三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的 Family Name 的首字符。 语法 awk [选项参数] ‘script’ var=value file(s) 或 awk [选项参数] -f scriptfile var=value file(s) 选项参数说明： -F fs or –field-separator fs指定输入文件折分隔符，fs是一个字符串或者是一个正则表达式，如-F:。 -v var=value or –asign var=value赋值一个用户定义变量。 -f scripfile or –file scriptfile从脚本文件中读取awk命令。 -mf nnn and -mr nnn对nnn值设置内在限制，-mf选项限制分配给nnn的最大块数目；-mr选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。 -W compact or –compat, -W traditional or –traditional在兼容模式下运行awk。所以gawk的行为和标准的awk完全一样，所有的awk扩展都被忽略。 -W copyleft or –copyleft, -W copyright or –copyright打印简短的版权信息。 -W help or –help, -W usage or –usage打印全部awk选项和每个选项的简短说明。 -W lint or –lint打印不能向传统unix平台移植的结构的警告。 -W lint-old or –lint-old打印关于不能向传统unix平台移植的结构的警告。 -W posix打开兼容模式。但有以下限制，不识别：/x、函数关键字、func、换码序列以及当fs是一个空格时，将新行作为一个域分隔符；操作符和=不能代替^和^=；fflush无效。 -W re-interval or –re-inerval允许间隔正则表达式的使用，参考(grep中的Posix字符类)，如括号表达式[[:alpha:]]。 -W source program-text or –source program-text使用program-text作为源代码，可与-f命令混用。 -W version or –version打印bug报告信息的版本。 例子123456789101112131415161718192021222324# 显示所有数据并在第一列加上行号awk '{print NR \"\\t\" $0}' test.txt# 显示所有数据并在第一列显示当前行的列数（空格分割为界）awk '{print NF \"\\t\" $0}' test.txt# 查找功能# 查找第一列为bob的数据awk '$1==\"bob\"{print $0}' test.txt# 查找第7行数据awk 'NR==7{print $0}' test.txt# 输入按逗号分割开，输出按制表符分割显示（输入输出分隔符默认都是空格）awk 'BEGIN{FS=\",\"; OFS=\"\\t\"} {print $1, $2}'# 将第3列的数据隐藏，显示为xxxawk '{$3=\"xxx\"; print $0}' data.txt# 打出文件的最后一列数据awk '{print $NF}' data.txtawk '{a=2; b=\"apple\"; c=3; print b+c}' # 输出3，字符串和数字相加会把字符串中的最前面数字部分才会相加，没有数字部分则为0awk '{a=2; b=\"32apple\"; c=3; print b+c}' # 输出35，只有最前面是数字才会相加 Regular Expression(正则表达式) 123456789101112131415161718192021222324252627282930313233343536# 匹配字符串中含有abc的值awk '/abc/{print $0}' data.txt# 匹配字符串中含有'a+一个任意字符+c'的值awk '/a.c/{print $0}' data.txt# 如果就要匹配'a.c'这个字符串就需要用转义字符awk '/a\\.c/{print $0}' data.txt# ^和$ 表示开始和结尾# 匹配必须以abc开始的值awk '/^abc/{print $0}' data.txt# 匹配必须以abc结尾的值awk '/abc$/{print $0}' data.txt# 匹配'a+a到z任意一个字符+c'awk '/a[a-z]c/{print $0}' data.txt# 匹配'a+除了小写a到z之外的一个字符+c'awk '/a[^a-z]c/{print $0}' data.txt# 匹配'0个a或者任意个a+b'awk '/a*b/{print $0}' data.txt# 匹配'至少一个a+b'awk '/a+b/{print $0}' data.txt# 匹配'a可以有也可以没有+b'awk '/a?b/{print $0}' data.txt# 匹配'abbbc'awk '/ab{3}c/{print $0}' data.txt# 匹配'a+3个b到10个b+c'awk '/ab{3,10}c/{print $0}' data.txt sedLinux sed 命令是利用脚本来处理文本文件。 sed 可依照脚本的指令来处理、编辑文本文件。 Sed 主要用来自动编辑一个或多个文件、简化对文件的反复操作、编写转换程序等。 语法 sed [-hnV][-e&lt;script&gt;][-f&lt;script文件&gt;][文本文件] 参数说明： -e&lt;script文件&gt;或–expression=&lt;script文件&gt; 以选项中指定的script来处理输入的文本文件。 -f&lt;script文件&gt;或–file=&lt;script文件&gt; 以选项中指定的script文件来处理输入的文本文件。 -h或–help 显示帮助。 -n或–quiet或–silent 仅显示script处理后的结果。 -V或–version 显示版本信息。 动作说明： a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p ：打印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～ s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ 例子将文本以行为单位加载进内存的模式空间，默认不编辑原文件，仅对模式空间中的数据做处理，之后再显示出来 1234567891011121314151617181920# 删除第1行和第2行(d表示删除)sed '1,2d' test.txt# 删除第3行到最后一行（$表示最后一行，$-1表示倒数第二行）sed '3,$d' test.txt# 模式匹配，删除包含oot的行sed '/oot/d' test.txt# 显示以/开始的行，会重复显示，因为模式空间还会显示一次，可加'-n'静默模式不显示模式空间的内容sed '/^\\//p' test.txt# 在以/开头的行后面加上hellosed -n '/^\\//a \\hello' test.txt# 把包含oot的行数据保存到etc/fstab中sed -n '/oot/w test.txt' /etc/fstab# 把文件中的oot替换成OOT(默认只替换每行中第一次被匹配的字符串，加了修饰g，全局替换)sed 's/oot/OOT/g' test.txt 几个例子1234567891011121314151617# 查找当前目录下包含127.0.0.1关键字的文件grep -rl '127.0.0.1' ./find . -type f |xargs grep \"127.0.0.1\"find . -type f -exec awk '/127.0.0.1/' {} \\;# 显示除过第3行到第10行的内容sed '3,10d' test.txtawk '!(NR&gt;=3&amp;&amp;NR&lt;=10)' test.txt# 显示第3行到第10行的内容sed -n '3,10p' test.txtawk 'NR&gt;=3&amp;&amp;NR&lt;=10' test.txthead -10 test.txt|tail -8awk 'NR==3,NR==7' test.txt # (逗号表示连续，第一次匹配到第二次匹配中间的所有行)# 只显示第3行和第10行的内容awk 'NR==3;NR==10' test.txt # (分号表示分隔符，分割多条命令) unsplash-logoAnnie Spratt","link":"/posts/dd355129/"},{"title":"终端利器之tmux篇","text":"","link":"/posts/7309d80/"},{"title":"终端利器之zsh篇","text":"对于开发者而言，终端的使用频率算是比较高的。其实，对我而言，日常中的大部分开发都是在终端中完成的，算得上是半个重度终端使用者了。恰好这次重新整理博客，所以打算写一个终端系列。主要包括终端美化和一些使用技巧。这篇文章主要介绍 zsh，后续会出文章介绍 vim 和 tmux。 这个系列所有的配置文件都在 github 仓库，感兴趣的自提，喜欢的不妨给个star~ 先放一张效果图 安装zsh安装Macos 默认安装了 zsh，以下为 centos 安装的命令 1234sudo yjm install zsh# 用户默认使用 zshsudo chsh -s /bin/zsh &lt;username&gt; oh-my-zsh安装zsh 本体有着强大的功能，但碍于其复杂的配置，对普通用户而言并不太适合。但是，一个开源项目的出现打破了这一局面 —— 它就是本文的主角：Oh My Zsh。借助 Oh My Zsh，你只需要进行极为简单的安装配置，就可以用上 zsh，并享受许多酷炫的功能，它的安装也很简单。 1sh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" 在安装过程中会提示 Do you want to change your default shell to zsh? [Y/n]（是否将默认 Shell 切换到 Zsh），按下 Y 并回车即可。随后会提示 Password for xxx，输入你的用户密码并回车即可。当你看见大大的 Oh my zsh 标志时就表示 Oh My Zsh 已经安装成功了。 配置目录结构欲善其功，必先利其器。了解 oh-my-zsh 的目录结构能极大的加深你对它的理解，也能让你对后续的配置更熟悉。 为了书写的方便，如无特别注明，后续所有的 zsh 都指 oh-my-zsh。 安装完 zsh 后，会自动在 $HOME 下生成默认配置文件 .zshrc。一般而言，用户只需要修改此文件即可。 zsh 的目录结构如下 1234567891011121314.oh-my-zsh├── CODE_OF_CONDUCT.md├── CONTRIBUTING.md├── LICENSE.txt├── README.md├── cache├── custom├── lib├── log├── oh-my-zsh.sh├── plugins├── templates├── themes└── tools 其中只有三个文件夹需要你关注 custom 第三方插件和主题的存放位置 1234custom├── example.zsh├── plugins└── themes plugins 和 themes 内置插件和主题存放位置 了解完目录，我们再看一下 zsh 加载插件和主题的过程。 首先看一下 .zshrc .zshrc 1source $ZSH/oh-my-zsh.sh 在设置完各种配置后，zsh 会先执行 oh-my-zsh.sh 脚本，这个才是真正加载你的配置的时候。我们把加载插件的过程拿出来看看，其他的加载过程都差不多 oh-my-zsh.sh1234567891011# Add all defined plugins to fpath. This must be done# before running compinit.for plugin ($plugins); do if is_plugin $ZSH_CUSTOM $plugin; then fpath=($ZSH_CUSTOM/plugins/$plugin $fpath) elif is_plugin $ZSH $plugin; then fpath=($ZSH/plugins/$plugin $fpath) else echo \"[oh-my-zsh] plugin '$plugin' not found\" fidone 可以很直观的看到，它会遍历你使用的所有插件，然后，它先去第三方插件目录下查找，没找到的话再去内置插件目录下查找。这样以来，我们就很直观的了解了，若想用第三方插件，只需要把它放到 custom/plugins 目录下即可。同样，主题也是如此。 有了上面的基础后，下面我们主要从美化的角度来进行配置。 美化配置美化主要包括两部分，主题和字体。 主题主题的设置在 .zshrc 中，修改下面内容为你喜欢的主题即可 .zshrc1ZSH_THEME=\"spaceship\" zsh有很多 内置主题 和 第三方主题，大家可以参考链接给的官方文档挑选喜欢的。 下面介绍的是我使用的主题 spaceship 的配置。 首先看一下配置完成的效果 关于这个主题的更多特性，想要了解更多的可以去官方了解。 安装 先 clone 主题 1git clone https://github.com/denysdovhan/spaceship-prompt.git \"$ZSH_CUSTOM/themes/spaceship-prompt\" 链接到第三方主题目录 1ln -s \"$ZSH_CUSTOM/themes/spaceship-prompt/spaceship.zsh-theme\" \"$ZSH_CUSTOM/themes/spaceship.zsh-theme\" 然后在 .zshrc 中设置主题，再刷新或者重启终端即可。 字体如果你是安装前面的步骤一步一步安装的，到这里你应该会发现出现了乱码的情况，这主要是字体不支持的缘故。这里，我们安装一下 powerline。 12345678# clonegit clone https://github.com/powerline/fonts.git --depth=1# installcd fonts./install.sh# clean-up a bitcd ..rm -rf fonts 再在你使用的终端中选择需要的字体，这里以 iterm2 为例。打开 Preferences-&gt;Profiles-&gt;Text-&gt;Change Font 选择字体，我这里使用的是 SauceCodePro Nerd Font。 插件接下来是关于我使用的插件，主要分为内置插件和三方插件。 插件的配置在 .zshrc 中 .zshrc123plugins=( git sudo vscode wakatime z zsh-autosuggestions extract web-search git-open zsh-syntax-highlighting ) 内置插件git为 git 添加了很多快捷键，比如 gaa : git add -A gc: git commit -v ggp: git push origin $(current_branch) ggl: git pull origin $(current_branch) 这里就不一一列举了，感兴趣的可以自行查看 .oh-my-zsh/plugins/git/README.md。 sudo通过按两下 esc 自动添加 sudo 前缀。 效果 z快速跳转，谁用谁知道。 效果 extract不用记那么多解压参数，一键解压，无脑操作。 效果 web-search终端打开浏览器搜索。 效果 三方插件git-open快速打开 git 仓库。 安装 1git clone https://github.com/paulirish/git-open.git $ZSH_CUSTOM/plugins/git-open 配置文件中插件下添加git-open 。 默认使用git open打开仓库，你可以重新设置一下快捷键，比如go，我因为使用golang所以换成了Go。 .zshrc1alias Go=\"git-open\" 效果 vscode在终端中使用 vscode 打开文件。 安装 1git clone https://github.com/valentinocossar/vscode.git $ZSH_CUSTOM/plugins/vscode 配置文件中插件下添加 vscode 。 使用 vs ：后接文件、目录或者路径。 组合参数 a：最后编辑的窗口 t：当前目录 s：sudo 所以你可以使用一下组合 vs：接文件、目录或路径，使用vs code打开 。 vsa：打开上次编辑的窗口。 vst：打开当前目录 随笔你组合。 效果 waketimewaketime 可以记录你的使用时长，同时还会记录你具体在那个项目上花了多少时间，最后还能分析你的使用习惯等待。当然，它基本在各个平台都有自己的插件。 想要使用这个插件，首先，你要去 waketime官网 申请一个账号，然后获取自己的密钥，然后再写入配置文件中。具体的操作流程，有需要或者兴趣的可以自行解决，官网或者百度都有教程，这里就不细说。 安装 1git clone https://github.com/sobolevn/wakatime-zsh-plugin.git wakatime $ZSH_CUSTOM/plugins/vscode 配置文件中插件下添加 waketime 。 效果 zsh-autosuggestions自动提示，五星好评，居家必备。 安装 1git clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM//plugins/zsh-autosuggestions 配置文件中插件下添加 zsh-autosuggestions 。 效果这里就不放了，看前面的那个图，大家应该就能感受的到。 zsh-syntax-highlighting语法高亮。 安装 1git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting 配置文件中插件下添加 zsh-syntax-highlighting ，这里需要注意，把这个插件写在最后面。 总结当然，以上只是我的配置方案。如果你想自己配置主题和插件，我相信看完这篇文章，对你来说应该都是小菜一碟。 unsplash-logoEsteban Amaro","link":"/posts/eec0bc4e/"},{"title":"软件工具篇","text":"","link":"/posts/fbcc746f/"},{"title":"终端利器之vim篇","text":"基本上，只要你使用过终端，多多少少应该都接触过 vim，但对大多数的新手来说，vim 的体验感绝对算不上好，甚至可以用糟糕来形容。但事实上，大部分人都没能体会到 vim 的牛逼之处就早早的放弃了，所以我们接上一篇，这次好好讲一讲 vim 的使用。 首先，vim 是什么？本质上来讲，vim 只是一个编译器，只不过以下的几个特性让它变得不可思议： 几乎在所有平台、ide 都内置 高度可定制化（包括大量的插件） 支持所有的编程语言 老规矩，文章分为三部分：安装、配置和插件。 先放效果图 安装对于几乎所有的 Linux 或者 Unix 系统，都内置安装了 vim，这里就不过多的介绍 vim 了，我们主要来介绍基于 vim 开发的另外一个东西——neovim，至于我为什么使用 neovim，主要是以下几点 全面并发虽说 vim 8 也开始支持了，但这当初应该是被 neovim 倒逼出来的。 默认配置更加友好说起来 vim 的一些过时默认配置会让你大吃一惊，比如它默认的 encoding 至今依然是 latin1 ！neovim 当然早改用 utf-8 了。还有一些不合时宜的默认 setting 也纷纷得到了修改，免去用户手动配置之苦，我就在迁移原 .vimrc 时删去了 25 行多，净化完毕，更加极简！ 充分遵循 XDG 规范vim 默认的 .vimrc 和 .vim 均一般在 $HOME 下，neovim 则全挪为 $XDG_CONFIG_HOME/nvim/init.vim 和 $XDG_CONFIG_HOME/nvim. vim 编辑文件时，可以有多达四个的数据文件：backup, swapfile, undofile 和 viminfo. Unix 下的 vim 分别默认存在 &quot;.,~/tmp,~/&quot;, &quot;.,~/tmp,/var/tmp,/tmp&quot;, &quot;.&quot;, 其中 viminfo 的具体储存位置我一时还查不出来，就懒得深究了。后来得知：undofile 默认没有值，不保存撤销记录，viminfo 位于用户目录。 neovim 则全改储存在 $XDG_DATA_HOME/nvim/ 下各自的目录里，此外 viminfo 更是被抛弃，被叫 ShaDa 且更为先进的二进制文件所代替，后者位于 $XDG_DATA_HOME/nvim/shada/main.shada。 再加上用户自行安装的 neovim 二进制包也在 $HOME/.local/bin 里，一家人更加整整齐齐。 至于如何安装，请自行解决。 配置我的配置目录如下 1234567891011121314151617181920~/.config/nvim➜ tree -L 2.├── autoload│ └── functions.vim├── colors│ ├── NeoSolarized.vim│ └── solarized.vim├── init.vim└── rc ├── 01.settings.vim ├── 02.plugin.vim ├── 03.airline.vim ├── 04.tabs.vim ├── 05.dev.vim ├── 06.tagbar.vim ├── 07.ctrlp.vim ├── 08.fzf.vim ├── 09.nerdtree.vim └── 10.startify.vim 刚安装时，默认的配置文件只有 init.vim，neovim 默认会加载它。这里我们先看看我的配置 init.vim123for f in split(glob('~/.config/nvim/rc/*.vim'), '\\n') exe 'source' fendfor 其实，思路很简单，真正的配置都放在 rc 目录下，这样做的目的是使配置文件更加的清晰简单，大概就是分为基础配置，插件安装，插件配置三大块，大家根据名字应该能很容易的理解 插件unsplash-logoM.Fildza Fadzil","link":"/posts/848c0324/"},{"title":"archlinuxa安装配置","text":"参考官方文档 安装前的准备设置字体刚进入系统，默认的字体特别小，用一下命令设置字体 1setfont /usr/share/kbd/consolefonts/LatGrkCyr-12x12.psfu.gz 验证启动模式输入以下命令查看启动模式 1ls /sys/firmware/efi/efivars 若有输出，则为 UEFI，否则为BOIS 连接网络有线1dhcpcd 无线1wifi-menu 或者到官方网络配置页面查找解决方式 更新系统时间1timedatectl set-ntp true 分区与格式化查看目前分区情况 1fdisk -l 引导分区 如果你是BIOS/MBR方式引导，无需创建引导分区。 如果你是EFI/GPT方式引导，并且同时安装了其他系统，那么你应该可以在分区列表中发现一个较小的并且类型为 EFI 的分区（注意查看硬盘的大小，这个EFI分区有可能是你U盘中的，需要排除），这是你的引导分区，请记下它的路径（/dev/sdxY)备用，创建引导分区的步骤。 如果你是EFI/GPT方式引导，但是没有这个较小的并且类型为EFI的引导分区（这种情况一般只会出现在新的硬盘），那么你需要先创建引导分区。 分区创建12345fdisk /dev/sdx # sdx 为你要操作的磁盘# 输入 m 查看帮助# 具体参数看帮助内容# 创建引导分区需要更改文件格式为 efi 格式化分区 格式化引导分区 1mkfs.fat -F32 /dev/sdxx # sdxx 为你刚刚创建的引导分区 格式化 swap 分区 12mkswap /dev/sdxy # sdxy 为你刚刚分区的 swap 分区swapon /dev/sdxy 格式化其他分区为 ext4 1mkfs.ext4 /dev/sdxz # sdxz 为你要格式化的分区，如 /，/home 等等 挂载分区 挂载根分区 1mount /dev/sdxz /mnt # sdxz 为根分区 挂载引导分区（如果有引导分区） 12mkdir /mnt/bootmount /dev/sdxx /mnt/boot # sdxx 为引导分区 其他分区 12mkdir /mnt/home # 以 home 分区为例，其他类似mount /dev/sdxz /mnt/home # sdxz 为 home 分区 安装选择镜像源编辑 /etc/pacman.d/mirrorlist 文件，将所有国内源剪切到文件开头。 同时可以编辑 etc/pacman.conf 文件，将 Color 取消注释，可以让 pacman 安装输出彩色。 安装基本包安装基本的软件包到磁盘上，需要联网 1pacstrap /mnt base base-devel linux linux-firmware dhcpcd 配置 Fstab生成自动挂载分区的 fstab 文件 1genfstab -L /mnt &gt;&gt; /mnt/etc/fstab ChrootChroot 意为 Change root，相当于把操纵权交给我们新安装（或已经存在）的Linux系统，执行了这步以后，我们的操作都相当于在磁盘上新装的系统中进行。 1arch-chroot /mnt 设置时区12ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimehwclock --systohc 提前安装必须软件1pacman -S neovim dialog wpa_supplicant ntfs-3g networkmanager 设置 Local编辑/etc/locale.gen文件，在文件中找到zh_CN.UTF-8 UTF-8 、 en_US.UTF-8 UTF-8这两行，去掉行首的#号，保存并退出。 然后执行 1locale-gen 创建 并编辑 /etc/locale.conf 文件 1234nvim /etc/locale.conf# 在第一行写入并保存LANG=en_US.UTF-8 设置主机名1234nvim /etc/hostname# 在文件的第一行输入你自己设定的一个myhostnamearch 123456nvim /etc/hosts# 在文件末尾写入127.0.0.1 localhost::1 localhost127.0.1.1 arch.localdomain arch 设置 Root 密码123passwd# 按提示设置并确认就可以了 引导配置 安装软件 1pacman -S intel-ucode os-prober ntfs-3g grub efibootmgr 部署 grub 1grub-install --target=x86_64-efi --efi-directory=/boot --bootloader-id=grub 生成配置文件 1grub-mkconfig -o /boot/grub/grub.cfg 重启12345678910# 退出exit# 前面挂载的分区全部卸载umount /mnt/bootumount /mnt/homeumount /mnt# 重启reboot 安装后这一部分主要讲安装完裸 archlinux 后配置桌面环境等内容。 我主要是使用窗口管理器，所以没有安装桌面环境，有需要的请自行安装。 网络12 systemctl enable dhcpcdsystemctl start dhcpcd 新建用户123456789useradd -m -G wheel username # username 换成你自己的 passwd usernameln -sf /usr/bin/nvim /usr/bin/vivisudo# 找到 # %wheel ALL=(ALL)ALL，取消注释reboot 图形化界面 xorg 1sudo pacman -S xf86-video-intel xorg xorg-xinit dwm &amp; menu &amp; st 123456git clone https://github.com/GallonHu/dwm.gitgit clone https://github.com/GallonHu/st.gitgit clone https://git.suckless.org/dmenu# 分别进入三个文件夹sudo make clean install yay 123git clone https://aur.archlinux.org/yay.gitcd yaymakepkg -si 字体 &amp; 表情 &amp; 图标 &amp; 主题 12yay -S ttf-linux-libertine ttf-inconsolata ttf-joypixels ttf-twemoji-color noto-fonts-emoji ttf-liberation ttf-droidyay -S wqy-bitmapfont wqy-microhei wqy-microhei-lite wqy-zenhei adobe-source-han-mono-cn-fonts adobe-source-han-sans-cn-fonts adobe-source-han-serif-cn-fonts nerd-fonts siji 软件包 1yay -S acpi acpitool adapta-gtk-theme alsa-utils feh google-chrome htop gnome-screenshot lxappearance nerd-fonts-source-code-pro network-manager-applet picom simplescreenrecorder screenkey thunar trayer tlp unzip vlc w3m xfce4-power-manager xfce4-volumed-pulse zip zsh lazygit ranger fzf unsplash-logoRod Long","link":"/posts/918ab41d/"}],"tags":[{"name":"NTP","slug":"NTP","link":"/tags/NTP/"},{"name":"RabbitMQ","slug":"RabbitMQ","link":"/tags/RabbitMQ/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"},{"name":"Install","slug":"Install","link":"/tags/Install/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Jupyter","slug":"Jupyter","link":"/tags/Jupyter/"},{"name":"Virtualenv","slug":"Virtualenv","link":"/tags/Virtualenv/"},{"name":"Markdown","slug":"Markdown","link":"/tags/Markdown/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"Shell","slug":"Shell","link":"/tags/Shell/"},{"name":"Zsh","slug":"Zsh","link":"/tags/Zsh/"},{"name":"Terminal","slug":"Terminal","link":"/tags/Terminal/"},{"name":"Vim","slug":"Vim","link":"/tags/Vim/"},{"name":"Archlinux","slug":"Archlinux","link":"/tags/Archlinux/"}],"categories":[{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Tutorials","slug":"Tutorials","link":"/categories/Tutorials/"},{"name":"Bigdata","slug":"Bigdata","link":"/categories/Bigdata/"}]}